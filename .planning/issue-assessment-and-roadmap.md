# VariantCentrifuge — Issue Assessment & Prioritized Roadmap

**Date:** 2026-02-13 (deep research refresh)
**Assessed by:** Claude Code (with web search)
**Repo:** scholl-lab/variantcentrifuge (v0.12.1)
**Branch:** main

---

## 1. Current State Assessment

### 1.1 Test Health

| Metric | Value |
|--------|-------|
| Tests collected | 1035 |
| Passing | 1035 |
| Skipped | 3 (missing optional test data) |
| Failures | 0 |
| Fast suite runtime | ~3 min (Windows), deselects 59 slow/integration |

All pytest markers registered in `pyproject.toml`. No collection errors.
Cross-platform: passes on both Windows and Linux (CI).

### 1.2 CI/CD

| Workflow | File | Purpose |
|----------|------|---------|
| **CI** | `test.yml` | Ruff lint + format, mypy type check, pytest on Python 3.10 + 3.12 with coverage |
| **Docker** | `docker.yml` | Multi-stage build, GHCR push, Trivy scanning, cosign signing |
| **Docs** | `docs.yml` | Sphinx build + GitHub Pages deploy |

### 1.3 Packaging & Reproducibility

| Item | Status |
|------|--------|
| `pyproject.toml` | Complete — hatchling, `requires-python>=3.10`, dev extras |
| `conda/environment.yml` | Fixed — named `variantcentrifuge`, all deps |
| `workflow/envs/variantcentrifuge.yml` | Snakemake conda env (external tools only) |
| Dockerfile | Multi-stage micromamba, GHCR CI |

### 1.4 Code Quality

| Tool | Status |
|------|--------|
| Ruff (lint + format) | 0 errors in `variantcentrifuge/` + `tests/` |
| mypy | Pre-existing pandas type stubs issues only |
| Pre-commit | ruff-pre-commit v0.11.0 enforced |

---

## 2. Issue Inventory

**All 30 historical issues are resolved.** Only 4 open issues remain, all
testing/performance infrastructure:

| # | Title | Category |
|---|-------|----------|
| 58 | Performance testing framework | Testing infrastructure |
| 60 | Real-world test datasets | Testing infrastructure |
| 61 | Comprehensive report generation validation | Testing infrastructure |
| 62 | Performance optimization | Performance |

---

## 3. Detailed Plans for Remaining Issues

### 3.1 — #60: Real-World Test Datasets

#### 3.1.1 Current Fixtures

The project already has two datasets in `tests/fixtures/`:
- **GIAB Ashkenazi Trio** (`giab/`) — HG002/HG003/HG004, 8 genes, GRCh37,
  joint-called HaplotypeCaller VCF + annotated version
- **Gene Burden synthetic data** (`geneburden/`) — 100 samples, 25 variants,
  7 genes, generated by Python script

**Gaps:** Single-sample germline benchmark, tumor-normal somatic pair,
multi-allelic edge cases, real multi-sample cohort for gene burden.

#### 3.1.2 GRCh38 Gene Coordinates for Subsetting

Selected for nephrology/rare disease relevance plus common benchmarking genes.
All coordinates from Ensembl GRCh38, with 5 kb padding for subsetting.

| Gene | Chr | Start | End | Size | Relevance |
|------|-----|-------|-----|------|-----------|
| BRCA1 | chr17 | 43,044,292 | 43,170,245 | ~126 kb | Cancer benchmark |
| BRCA2 | chr13 | 32,315,086 | 32,400,268 | ~85 kb | Cancer benchmark |
| TP53 | chr17 | 7,668,402 | 7,687,550 | ~19 kb | Tumor suppressor |
| CFTR | chr7 | 117,287,120 | 117,715,971 | ~429 kb | Cystic fibrosis |
| PKD1 | chr16 | 2,088,708 | 2,135,898 | ~47 kb | Polycystic kidney disease |
| PKD2 | chr4 | 88,007,629 | 88,077,777 | ~70 kb | Polycystic kidney disease |
| COL4A3 | chr2 | 227,164,624 | 227,314,792 | ~150 kb | Alport syndrome (AR) |
| COL4A4 | chr2 | 227,002,714 | 227,164,453 | ~162 kb | Alport syndrome (AR) |
| COL4A5 | chrX | 108,439,838 | 108,697,545 | ~258 kb | Alport syndrome (X-linked) |

**BED file (GRCh38, 5 kb padded):**
```
chr17	43039292	43175245	BRCA1
chr13	32310086	32405268	BRCA2
chr17	7663402	7692550	TP53
chr7	117282120	117720971	CFTR
chr16	2083708	2140898	PKD1
chr4	88002629	88082777	PKD2
chr2	227159624	227319792	COL4A3
chr2	226997714	227169453	COL4A4
chrX	108434838	108702545	COL4A5
```

Note: COL4A3 and COL4A4 are adjacent on chr2 and can be merged: `chr2:226997714-227319792`.

#### 3.1.3 Recommended Datasets

| # | Scenario | Source | Approach | Est. Size |
|---|----------|--------|----------|-----------|
| 1 | Single-sample germline | GIAB CMRG v1.00 HG002 | Direct download, no subsetting | **226 KB** |
| 2 | Trio (inheritance) GRCh38 | GIAB v4.2.1 Ashkenazi | Subset + merge 3 per-sample VCFs | **~1-2 MB** |
| 3 | Multi-sample cohort | 1000 Genomes 30x GRCh38 | Subset 10 samples x 9 genes | **~1-5 MB** |
| 4 | Tumor-normal pair | Synthetic Mutect2 VCF | Python generator script | **~50-200 KB** |
| 5 | Multi-allelic sites | 1000G extract + hand-crafted | `bcftools view -m 3` + edge cases | **~50-100 KB** |

**Total: under 10 MB** (fits in GitHub).

#### 3.1.4 Dataset Details

**Dataset 1 — GIAB CMRG (Challenging Medically Relevant Genes)**

The CMRG benchmark contains variants in 273 medically relevant genes including
segmental duplications (PMS2, SMN1, STRC, PKD1). Already 226 KB — no subsetting needed.

- VCF (GRCh38): `https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/CMRG_v1.00/GRCh38/SmallVariant/HG002_GRCh38_CMRG_smallvar_v1.00.vcf.gz` (226 KB)
- Index: same path with `.tbi` (14 KB)
- BED: same dir, `HG002_GRCh38_CMRG_smallvar_v1.00.bed` (60 KB)
- Contents: ~17,000 SNVs + ~3,600 indels
- License: Public domain (NIST, US government work)
- Reference: [Wagner et al. 2022](https://pmc.ncbi.nlm.nih.gov/articles/PMC9117392/)

**Dataset 2 — GIAB Ashkenazi Trio (GRCh38, gene subset)**

Individual per-sample benchmark VCFs are ~140-149 MB each; subsetting required.

| Sample | Role | URL |
|--------|------|-----|
| HG002 | Son (proband) | `https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz` |
| HG003 | Father | same pattern under `HG003_NA24149_father/` |
| HG004 | Mother | same pattern under `HG004_NA24143_mother/` |

Subsetting procedure:
```bash
# For each sample: subset to gene panel regions
for SAMPLE in HG002 HG003 HG004; do
  bcftools view -R gene_panel_GRCh38.sorted.bed \
    ${SAMPLE}_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \
    -Oz -o ${SAMPLE}_GRCh38_gene_subset.vcf.gz
  tabix -p vcf ${SAMPLE}_GRCh38_gene_subset.vcf.gz
done

# Merge into multi-sample trio VCF
bcftools merge -0 -m none \
  HG002_GRCh38_gene_subset.vcf.gz \
  HG003_GRCh38_gene_subset.vcf.gz \
  HG004_GRCh38_gene_subset.vcf.gz \
  -Oz -o GIAB_trio_GRCh38_gene_subset.vcf.gz
tabix -p vcf GIAB_trio_GRCh38_gene_subset.vcf.gz
```

Important: GIAB NISTv4.2.1 GRCh38 VCFs use `chr` prefix — BED must match.

**Dataset 3 — 1000 Genomes Multi-Sample Cohort**

Two options available:

*Option A: Phase 3 (GRCh37, well-established)*
- FTP: `https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/`
- Per-chromosome: `ALL.chr{N}.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz`
- 2,504 unrelated samples, no `chr` prefix

*Option B: 30x High-Coverage (GRCh38, newer, 602 trios)*
- FTP: `http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/20220422_3202_phased_SNV_INDEL_SV/`
- Per-chromosome: `1kGP_high_coverage_Illumina.chr{N}.filtered.SNV_INDEL_SV_phased_panel.vcf.gz`
- 3,202 samples with pedigree metadata, `chr` prefix
- Reference: [Byrska-Bishop et al. 2022](https://www.sciencedirect.com/science/article/pii/S0092867422009916)

```bash
# 10 diverse, unrelated samples
cat > samples_10.txt <<'EOF'
NA12878
NA19239
NA20502
HG00096
HG00268
NA18525
NA19648
HG01882
NA20845
HG03052
EOF

# Subset per chromosome, then concatenate
for CHR in 2 4 7 13 16 17; do
  bcftools view -R gene_panel.bed -S samples_10.txt \
    ALL.chr${CHR}....vcf.gz -Oz -o chr${CHR}_subset.vcf.gz
  tabix -p vcf chr${CHR}_subset.vcf.gz
done
bcftools concat chr{2,4,7,13,16,17}_subset.vcf.gz -Oz -o 1kgp_cohort_subset.vcf.gz
```

License: Fort Lauderdale agreement (public, unrestricted).

**Dataset 4 — Synthetic Tumor-Normal (Mutect2)**

Python generator producing valid Mutect2 VCF with:
- Meta-lines: `##tumor_sample=TUMOR`, `##normal_sample=NORMAL`, `##source=Mutect2`
- Sample order: GEN[0]=TUMOR, GEN[1]=NORMAL
- FORMAT fields: GT, AD, AF, DP, F1R2, F2R1, SB
- Variant categories:
  - Clear somatic: tumor AF 0.05-0.50, normal AF 0.00-0.02, PASS
  - Germline contamination: both AF > 0.20, `germline` filter
  - Weak evidence: tumor AF 0.02-0.05, `weak_evidence` filter
  - LOH: normal 0/1, tumor 1/1
  - Multi-allelic somatic: two ALT alleles in tumor
- Target genes: TP53, BRCA1, BRCA2, KRAS, EGFR, PIK3CA

**Dataset 5 — Multi-Allelic Edge Cases**

Hand-crafted VCF covering:
1. Biallelic at multi-allelic site: `REF=A, ALT=C,T` with GT `0/1`, `1/2`, `0/2`
2. Mixed SNV + indel at same site: `REF=AT, ALT=A,ATT,CT`
3. Spanning deletion: `ALT=*`
4. Overlapping variants at same position
5. High-ploidy multi-allelic: `GT=0/1/2` (edge case)

Plus: `bcftools view -m 3 -r chr17:43044292-43170245` from 1000G for real examples.

#### 3.1.5 Reproducibility Requirements

Every dataset accompanied by:
1. Download/generation script in `tests/fixtures/scripts/`
2. SHA256 checksums in `checksums.sha256` manifest
3. Pinned URLs with version numbers (not "latest")
4. README documenting provenance and license

#### 3.1.6 Priority Order

1. Dataset 1 (GIAB CMRG) — single `curl` command, no processing
2. Dataset 4 (Synthetic Mutect2) — Python script, no external downloads
3. Dataset 5 (Multi-allelic) — hand-craft, minimal effort
4. Dataset 2 (GIAB Trio GRCh38) — requires ~430 MB download + subsetting
5. Dataset 3 (1000G Cohort) — requires large per-chromosome downloads

Datasets 1, 4, 5 can be prepared immediately. Datasets 2, 3 require Linux/macOS with bcftools.

#### 3.1.7 Existing Test Dataset Repos (Reference)

| Repository | Contents |
|-----------|----------|
| [Ensembl VEP examples/](https://github.com/Ensembl/ensembl-vep) | `homo_sapiens_GRCh38.vcf` (~100 variants) |
| [davetang/learning_vcf_file](https://github.com/davetang/learning_vcf_file) | bcftools, HaplotypeCaller, freebayes examples |
| [vcflib/samples/](https://github.com/vcflib/vcflib) | Multi-sample, multi-allelic VCF |
| [DeepVariant quickstart](https://github.com/google/deepvariant) | chr20 100kb subset with truth VCF |
| GATK tutorials (`gs://gatk-tutorials/`) | HCC1143 tumor/normal Mutect2 example data |

#### 3.1.8 Key References

- [NIST Genome in a Bottle](https://www.nist.gov/programs-projects/genome-bottle)
- [GIAB CMRG v1.00 FTP](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/CMRG_v1.00/)
- [GIAB HG002 v4.2.1 GRCh38 FTP](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38/)
- [Wagner et al. 2022 — CMRG benchmark](https://pmc.ncbi.nlm.nih.gov/articles/PMC9117392/)
- [Byrska-Bishop et al. 2022 — 1000G 30x](https://www.sciencedirect.com/science/article/pii/S0092867422009916)
- [1000 Genomes Phase 3 FTP](https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/)
- [IGSR 30x GRCh38 Data Portal](https://www.internationalgenome.org/data-portal/data-collection/30x-grch38)
- [bcftools Manual](https://samtools.github.io/bcftools/bcftools.html)
- [GATK Mutect2 Documentation](https://gatk.broadinstitute.org/hc/en-us/articles/360037593851-Mutect2)
- [VCF Specification v4.5](http://samtools.github.io/hts-specs/VCFv4.5.pdf)

---

### 3.2 — #58: Performance Testing Framework

#### 3.2.1 Tool Stack

| Tool | Purpose | Platform | Notes |
|------|---------|----------|-------|
| **pytest-benchmark** v5.2+ | Per-function timing, pedantic mode, JSON output | All | Primary |
| **tracemalloc** (stdlib) | Deterministic memory budget assertions | All | Cross-platform |
| **pytest-memray** v1.7+ | C-extension-aware memory tracking, `@limit_memory` | Linux only | CI |
| **github-action-benchmark** | Historical tracking, PR regression alerts, GitHub Pages charts | CI | Visualization |

#### 3.2.2 pytest-benchmark: Pedantic Mode

Pedantic mode is **essential** for bioinformatics workloads. It disables automatic
calibration that would otherwise run expensive DataFrame operations hundreds of times.

```python
@pytest.mark.benchmark(group="inheritance-analysis")
def test_inheritance_analysis_performance(benchmark, variant_df_10k):
    from variantcentrifuge.inheritance.analyzer import analyze_inheritance

    def setup():
        return (variant_df_10k.copy(), config, pedigree), {}

    result = benchmark.pedantic(
        analyze_inheritance,
        setup=setup,
        rounds=5,           # 5 rounds for slow benchmarks
        warmup_rounds=1,    # 1 warmup to stabilize caches
        iterations=1,       # 1 iteration per round (function is slow)
    )
    assert "inheritance_pattern" in result.columns
```

#### 3.2.3 Benchmark Groups and Scaling Tests

```python
@pytest.mark.benchmark(group="genotype-replacement")
@pytest.mark.parametrize("n_variants", [100, 1000, 10000], ids=["100v", "1kv", "10kv"])
def test_replacement_scaling(benchmark, n_variants):
    df = make_variant_df(n_variants)
    benchmark.pedantic(
        replace_genotypes, args=(df, config),
        rounds=5, warmup_rounds=1, iterations=1,
    )
```

Extra metadata tracking:
```python
benchmark.extra_info["n_variants"] = 10000
benchmark.extra_info["n_genes"] = 50
benchmark.extra_info["peak_memory_mb"] = peak / 1024 / 1024
```

#### 3.2.4 CI Integration: Flake-Free Assertions

GitHub Actions runners have ~2.66% coefficient of variation ([Quansight analysis](https://labs.quansight.org/blog/github-actions-benchmarks),
[Akinshin measurements](https://aakinshin.net/posts/github-actions-perf-stability/)).
With a 2% performance gate, false positive rate is ~45%.

**Strategy 1: One-Sided Performance Canaries** ([Solidean pattern](https://solidean.com/blog/2025/practical-performance-tests/))

Pass on first run that meets target; fail only if ALL attempts miss. Catches 20%+
regressions without flaking on CI noise.

```python
def performance_canary(run_fn, *, min_throughput, max_attempts=10, unit="items/sec"):
    measurements = []
    for _ in range(max_attempts):
        start = time.perf_counter()
        count = run_fn()
        elapsed = time.perf_counter() - start
        throughput = count / elapsed if elapsed > 0 else 0
        measurements.append(throughput)
        if throughput >= min_throughput:
            return  # PASS
    best = max(measurements)
    raise AssertionError(
        f"Canary FAILED: target={min_throughput:.0f} {unit}, "
        f"best={best:.0f} {unit} over {max_attempts} attempts"
    )
```

**Strategy 2: Ratio Assertions** (zero flakiness)

Compare within the same CI run — eliminates machine-to-machine variance entirely.

```python
def test_vectorized_faster_than_sequential(variants_10k):
    t_seq = measure(replace_sequential, variants_10k)
    t_vec = measure(replace_vectorized, variants_10k)
    assert t_seq / t_vec >= 3.0

def test_linear_scaling():
    t_5k = measure(analyze, create_df(5_000))
    t_10k = measure(analyze, create_df(10_000))
    assert t_10k / t_5k < 3.0  # Linear=2x, allow overhead
```

**Strategy 3: Memory Budgets** (deterministic via tracemalloc)

```python
class MemoryBudget:
    def __init__(self, max_mb):
        self.max_mb = max_mb
        self.peak_mb = 0.0

    def __enter__(self):
        tracemalloc.start()
        return self

    def __exit__(self, *args):
        _, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        self.peak_mb = peak / 1024 / 1024
        assert self.peak_mb < self.max_mb, (
            f"Peak {self.peak_mb:.1f} MB exceeded budget {self.max_mb} MB"
        )
```

**Strategy 4: Historical Tracking** (alert, don't fail)

Use `github-action-benchmark` with 150% threshold on main-branch merges only.

#### 3.2.5 CI Workflow: github-action-benchmark

```yaml
# .github/workflows/benchmark.yml
name: Performance Benchmarks
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

permissions:
  contents: write
  pull-requests: write

concurrency:
  group: benchmark-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true
      - run: uv pip install --system -e ".[dev]" pytest-benchmark

      - name: Run benchmarks
        run: |
          python -m pytest tests/performance/ \
            -m "benchmark and not slow" \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --benchmark-min-rounds=3 \
            --benchmark-warmup=on \
            --benchmark-disable-gc -v

      - name: Store results (main)
        if: github.ref == 'refs/heads/main'
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: "Variantcentrifuge Performance"
          tool: "pytest"
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: "150%"
          comment-on-alert: true
          fail-on-alert: false
          alert-comment-cc-users: "@berntpopp"
          benchmark-data-dir-path: "dev/bench"
          summary-always: true

      - name: Compare PR benchmarks
        if: github.event_name == 'pull_request'
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: "Variantcentrifuge Performance"
          tool: "pytest"
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          alert-threshold: "150%"
          comment-on-alert: true
          fail-on-alert: true
          auto-push: false
          summary-always: true
```

Charts available at: `https://scholl-lab.github.io/variantcentrifuge/dev/bench/`

#### 3.2.6 Memory Profiling

**Linux CI: pytest-memray** (tracks C extensions, declarative markers)

```python
@pytest.mark.limit_memory("50 MB")
def test_inheritance_analysis_memory(variant_df_10k):
    result = analyze_inheritance(variant_df_10k, config, pedigree)

@pytest.mark.limit_leaks("1 MB")
def test_no_memory_leaks_in_scoring():
    for _ in range(100):
        score_variants(df, scoring_config)
```

**Cross-platform: tracemalloc fixture** (works on Windows)

```python
@pytest.fixture
def memory_budget():
    return MemoryBudget  # Usage: with memory_budget(max_mb=200): ...
```

**Combined speed + memory benchmark:**

```python
def test_inheritance_perf_and_memory(benchmark):
    def run_with_memory():
        tracemalloc.start()
        result = analyze_inheritance(df, config, pedigree)
        _, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        benchmark.extra_info["peak_memory_mb"] = peak / 1024 / 1024
        return result

    result = benchmark.pedantic(run_with_memory, rounds=3, warmup_rounds=1, iterations=1)
    assert benchmark.extra_info["peak_memory_mb"] < 100
```

#### 3.2.7 Synthetic Data Generator

```python
# tests/performance/conftest.py
def make_variant_dataframe(
    n_variants: int = 1000, n_samples: int = 3,
    n_genes: int = 10, seed: int = 42,
) -> pd.DataFrame:
    rng = np.random.default_rng(seed)
    genes = [f"GENE{i}" for i in range(n_genes)]
    samples = [f"sample_{i}" for i in range(n_samples)]
    chroms = [str(c) for c in range(1, 23)] + ["X", "Y"]

    gt_options = ["0/0", "0/1", "1/0", "1/1", "./."]
    gt_weights = [0.5, 0.25, 0.05, 0.15, 0.05]
    gt_strings = []
    for _ in range(n_variants):
        gts = rng.choice(gt_options, size=n_samples, p=gt_weights)
        gt_strings.append(";".join(f"{s}({g})" for s, g in zip(samples, gts)))

    effects = rng.choice(
        ["missense_variant", "synonymous_variant", "frameshift_variant",
         "stop_gained", "splice_donor_variant", "intron_variant"],
        size=n_variants, p=[0.3, 0.25, 0.1, 0.05, 0.05, 0.25],
    )

    df = pd.DataFrame({
        "CHROM": rng.choice(chroms, n_variants),
        "POS": rng.integers(10000, 250000000, n_variants),
        "REF": rng.choice(list("ACGT"), n_variants),
        "ALT": rng.choice(list("ACGT"), n_variants),
        "GENE": rng.choice(genes, n_variants),
        "GT": gt_strings,
        "AF": rng.exponential(0.01, n_variants).clip(0, 1),
        "ANN[0].EFFECT": effects,
        "FILTER": ["PASS"] * n_variants,
        "QUAL": rng.uniform(10, 1000, n_variants),
    })
    return df.sort_values(["CHROM", "POS"]).reset_index(drop=True)
```

Fixtures at standard sizes: 100, 1K, 10K, 50K in `tests/performance/conftest.py`.

#### 3.2.8 Alternative Tools Considered

| Tool | Verdict |
|------|---------|
| **[ASV](https://github.com/airspeed-velocity/asv)** | Better for libraries with thousands of users; overkill for this project |
| **[CodSpeed](https://codspeed.io/)** | 0.56% variance, 0.04% false alarm rate; excellent future upgrade for OSS |
| **[pyperf](https://github.com/psf/pyperf)** | Gold standard for microbenchmarks; manual investigation, not CI |
| **[Scalene](https://github.com/plasma-umass/scalene)** | Line-level CPU+memory profiling; development-time only |
| **[py-spy](https://github.com/benfred/py-spy)** | Zero-overhead sampling, `--subprocesses` flag for bcftools/SnpSift profiling |

#### 3.2.9 File Structure

```
tests/performance/
  conftest.py                     # Fixtures, factories, canary helper, memory budget
  test_bench_inheritance.py       # Inheritance analysis benchmarks
  test_bench_scoring.py           # Variant scoring benchmarks
  test_bench_genotype_replace.py  # Genotype replacement benchmarks
  test_bench_gene_burden.py       # Gene burden analysis benchmarks
  test_bench_dataframe_ops.py     # DataFrame I/O benchmarks

.github/workflows/
  benchmark.yml                   # Benchmark CI workflow
```

#### 3.2.10 Dependencies

```toml
# pyproject.toml [project.optional-dependencies]
bench = [
    "pytest-benchmark>=5.2",
]
```

#### 3.2.11 Implementation Steps

| # | Action | Effort |
|---|--------|--------|
| 1 | Add `pytest-benchmark` to dev deps | 5 min |
| 2 | Create `tests/performance/conftest.py` with factories + helpers | 30 min |
| 3 | Benchmark vectorized replacer, comp_het, inheritance, scoring | 1-2 hr |
| 4 | Add canary + ratio + complexity assertions to CI | 1 hr |
| 5 | Add `tracemalloc` memory budget tests | 1 hr |
| 6 | Add `benchmark.yml` workflow with github-action-benchmark | 30 min |
| 7 | Initialize gh-pages branch for chart storage | 10 min |

#### 3.2.12 Key References

- [pytest-benchmark documentation](https://pytest-benchmark.readthedocs.io/en/latest/)
- [pytest-benchmark pedantic mode](https://pytest-benchmark.readthedocs.io/en/latest/pedantic.html)
- [github-action-benchmark](https://github.com/benchmark-action/github-action-benchmark)
- [pytest-memray](https://github.com/bloomberg/pytest-memray)
- [Solidean: Practical CI-friendly Performance Tests](https://solidean.com/blog/2025/practical-performance-tests/)
- [GitHub Actions benchmark stability](https://aakinshin.net/posts/github-actions-perf-stability/)
- [CodSpeed: benchmarks without noise](https://codspeed.io/blog/benchmarks-in-ci-without-noise)
- [Quansight: Is GitHub Actions suitable for benchmarks?](https://labs.quansight.org/blog/github-actions-benchmarks)
- [Building continuous benchmarking in bioinformatics (PLOS)](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1013658)
- [vcf-bench](https://github.com/brentp/vcf-bench) — cyvcf2 6.9x faster than pysam

---

### 3.3 — #61: Report Generation Validation

#### 3.3.1 Tool Stack

| Purpose | Library | Notes |
|---------|---------|-------|
| HTML DOM parsing | **BeautifulSoup4** (`html.parser`) | Query by id/class/tag, no lxml needed |
| Snapshot regression | **syrupy** v5.1+ | `assert x == snapshot`, custom serializers |
| Excel inspection | **openpyxl** (already a dep) | Cells, styles, hyperlinks, sheets |
| IGV subprocess mock | **unittest.mock** | Avoid running real `create_report` |

Dependencies to add to `[dev]`: `beautifulsoup4>=4.12`, `syrupy>=5.0`.

#### 3.3.2 HTML Report Validation (BeautifulSoup)

**What to test:**
- HTML is parseable (BeautifulSoup does not raise)
- `<table id="variants_table">` exists with correct class
- `<thead>` column count matches input
- `<tbody>` row count matches variant count
- External links have `href` starting with `https://` and `target="_blank"`
- Summary stats cards contain expected numbers
- Empty variants render alert div instead of table
- CDN script tags for jQuery, DataTables, Plotly exist
- `<html lang="en">` present
- All images have `alt` text

**What to skip:** DataTables runtime behavior, Plotly chart rendering,
CSS appearance, CDN availability.

**Pattern — render template and parse:**
```python
from bs4 import BeautifulSoup

class TestHTMLReportStructure:
    def test_table_row_count(self, generated_html):
        table = generated_html.find("table", id="variants_table")
        rows = table.find("tbody").find_all("tr")
        assert len(rows) == 2

    def test_external_links_target_blank(self, generated_html):
        table = generated_html.find("table", id="variants_table")
        for link in table.find_all("a", href=re.compile(r"^https?://")):
            assert link.get("target") == "_blank"

    def test_empty_links_not_rendered(self, generated_html):
        for link in generated_html.find_all("a"):
            assert link.get("href", "").strip() != ""

class TestHTMLExternalLinks:
    def test_spliceai_link_format(self, generated_html):
        links = generated_html.find_all("a", href=re.compile(r"spliceailookup\.broadinstitute"))
        assert len(links) > 0

    def test_clinvar_link_format(self, generated_html):
        links = generated_html.find_all("a", href=re.compile(r"ncbi\.nlm\.nih\.gov/clinvar"))
        assert len(links) > 0
```

#### 3.3.3 Snapshot Testing (syrupy)

Custom HTML extension that normalizes dynamic content before comparison:

```python
class HTMLSnapshotExtension(AmberSnapshotExtension):
    def serialize(self, data, **kwargs):
        if isinstance(data, str):
            data = re.sub(r"Generated on \d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}",
                          "Generated on YYYY-MM-DD HH:MM:SS", data)
            data = re.sub(r"VariantCentrifuge v\d+\.\d+\.\d+",
                          "VariantCentrifuge vX.Y.Z", data)
            data = re.sub(r"[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}",
                          "UUID-NORMALIZED", data)
        return super().serialize(data, **kwargs)
```

Update with `pytest --snapshot-update`. Commit `.ambr` files.

Tip: Snapshot only structural sections (e.g., `<thead>`) rather than full documents
to reduce snapshot file sizes and maintenance burden.

#### 3.3.4 Excel Report Validation (openpyxl)

**What to test:**
- Expected sheets exist: "Results", "Metadata", "Statistics", "Gene Burden"
- Header row has correct column names
- Data row count matches source TSV
- Frozen panes at `A2`, auto-filter enabled
- URL columns have `.hyperlink` set
- Round-trip: `pd.read_excel(xlsx)` matches `pd.read_csv(tsv)` for key columns

```python
from openpyxl import load_workbook

class TestExcelFormatting:
    def test_freeze_panes(self, sample_tsv, sample_config):
        xlsx_path = convert_to_excel(str(sample_tsv), sample_config)
        finalize_excel_file(xlsx_path, sample_config)
        wb = load_workbook(xlsx_path)
        for ws in wb.worksheets:
            assert ws.freeze_panes == "A2"

    def test_auto_filter_enabled(self, sample_tsv, sample_config):
        xlsx_path = convert_to_excel(str(sample_tsv), sample_config)
        finalize_excel_file(xlsx_path, sample_config)
        wb = load_workbook(xlsx_path)
        ws = wb["Results"]
        assert ws.auto_filter.ref is not None

    def test_url_hyperlinks(self, sample_tsv_with_links, sample_config):
        xlsx_path = convert_to_excel(str(sample_tsv_with_links), sample_config)
        finalize_excel_file(xlsx_path, sample_config)
        wb = load_workbook(xlsx_path)
        ws = wb["Results"]
        headers = [cell.value for cell in ws[1]]
        if "SpliceAI" in headers:
            splice_col = headers.index("SpliceAI") + 1
            assert ws.cell(row=2, column=splice_col).hyperlink is not None
```

**What to skip:** Exact cell widths, font sizes, color hex values, byte-level comparison.

#### 3.3.5 IGV Report Validation

**What to test (via mocking):**
- `create_report` command assembled with correct flags
- Per-variant TSV files written with correct columns
- `igv_reports_map.json` structure: variant entries have chrom/pos/ref/alt/sample_reports
- Report paths are relative, not absolute
- Samples with `0/0` or `./.` excluded
- Failed subprocess returns `(False, ...)`, no crash

```python
class TestIGVReportMapStructure:
    def test_variant_entry_structure(self, igv_map):
        for variant in igv_map["variants"]:
            assert all(k in variant for k in ["chrom", "pos", "ref", "alt", "sample_reports"])
            assert isinstance(variant["sample_reports"], dict)

    def test_report_paths_are_relative(self, igv_map):
        for variant in igv_map["variants"]:
            for path in variant["sample_reports"].values():
                assert not Path(path).is_absolute()
                assert path.endswith("_igv_report.html")
```

**What to skip:** Actual `create_report` output, IGV.js rendering, BAM content.

#### 3.3.6 Edge Cases to Cover

- Empty variant set (header-only TSV) → "no variants" message, no table
- Single variant with all fields populated
- Multi-allelic with very long REF/ALT (>100 chars)
- Special characters in HGVS notation
- Missing optional columns (no GENE, no IMPACT)
- Unicode in sample names

#### 3.3.7 Testing Patterns from Bioinformatics

| Tool | Pattern |
|------|---------|
| **MultiQC** | E2E smoke test (tool runs without error) + isolated unit tests with type annotations |
| **GATK** | Every bug fix requires a regression test; every tool needs at least one E2E integration test |
| **VEP** | Golden file tests for expected output; regression tests for bug fixes |

#### 3.3.8 Test Pyramid for Reports

```
                    /\
                   /  \        Manual: Visual review, axe DevTools
                  /    \
                 /------\
                /        \     Integration: Full pipeline E2E report generation
               /          \    (test_report_end_to_end.py)
              /------------\
             /              \   Unit: Structural validation per report type
            /                \  (test_html_*, test_excel_*, test_igv_*)
           /                  \
          /--------------------\
         /                      \ Snapshot: Regression detection
        /                        \ (test_html_report_snapshots.py)
       /----------------------------\
```

Recommendation: structural validation as primary (explicit, tolerant of cosmetic changes);
snapshot tests for specific sections where regression detection matters most.

#### 3.3.9 File Structure

```
tests/
  unit/
    test_html_report_validation.py     # BeautifulSoup structural tests
    test_html_report_snapshots.py      # Syrupy regression tests
    test_excel_report_validation.py    # openpyxl workbook tests
    test_igv_report_content.py         # IGV map and HTML structure
  integration/
    test_report_end_to_end.py          # Full pipeline reports
  conftest_snapshots.py                # HTMLSnapshotExtension
```

#### 3.3.10 Implementation Priority

1. **High**: `test_html_report_validation.py` — most user-facing output
2. **High**: `test_excel_report_validation.py` — multi-sheet structure, formatting
3. **Medium**: `test_igv_report_content.py` — IGV map JSON structure
4. **Medium**: Extend existing integration tests with output validation
5. **Low**: `test_html_report_snapshots.py` — add after structural tests are stable
6. **Low**: Accessibility checks (lang, alt text, table semantics)

#### 3.3.11 Key References

- [BeautifulSoup Documentation](https://beautiful-soup-4.readthedocs.io/en/latest/)
- [Syrupy Documentation](https://syrupy-project.github.io/syrupy/)
- [Syrupy GitHub](https://github.com/syrupy-project/syrupy)
- [openpyxl Tutorial](https://openpyxl.readthedocs.io/en/latest/tutorial.html)
- [igv-reports GitHub](https://github.com/igvteam/igv-reports)
- [MultiQC GitHub](https://github.com/MultiQC/MultiQC)
- [GATK GitHub](https://github.com/broadinstitute/gatk)

---

### 3.4 — #62: Performance Optimization

#### 3.4.1 Prerequisite

Phase 3.2 (#58 benchmarks) must be complete before optimization work.
All changes must be validated against baselines. **Profile before optimizing.**

#### 3.4.2 Profiling First

**Scalene** (line-level CPU + memory, 10-20% overhead):
```bash
scalene --cpu --memory --outfile profile.html -- python -m variantcentrifuge.cli \
    --use-new-pipeline --vcf test.vcf.gz --genes "BRCA1 BRCA2"
```

**py-spy** (zero-overhead sampling, captures subprocess time):
```bash
py-spy record --subprocesses -o flame.svg -- python -m variantcentrifuge.cli \
    --use-new-pipeline --vcf test.vcf.gz --genes "BRCA1"
```

**Built-in stage timing** (add to pipeline runner):
```python
# variantcentrifuge/pipeline_core/runner.py
import time
for stage in self.stages:
    start = time.perf_counter()
    stage.run(context)
    elapsed = time.perf_counter() - start
    logger.info(f"Stage {stage.name}: {elapsed:.2f}s")
```

#### 3.4.3 Tiered Optimization Roadmap

**Tier 1: Quick Wins (1-2 days, immediate impact)**

| Change | Effort | Impact | Risk | Files |
|--------|--------|--------|------|-------|
| Add `engine="pyarrow"` to hot-path `read_csv` | Very Low | 10-40x faster CSV I/O | Low | `vectorized_replacer.py`, `stages/analysis_stages.py`, `analyze_variants.py` |
| Add `observed=True, sort=False` to all `groupby("GENE")` | Very Low | Avoids 3500x slowdown with categoricals | Low | `analyzer.py`, `parallel_analyzer.py`, `gene_burden.py`, `stats.py` |
| Replace `iterrows` with `itertuples` in inheritance Pass 3 | Low | ~10x faster iteration | Very Low | `analyzer.py:138`, `parallel_analyzer.py:254` |
| Add `gc.collect()` between pipeline stages | Very Low | Frees intermediate memory | Very Low | `pipeline_core/runner.py` |

**Tier 2: Medium-Term (1-2 weeks)**

| Change | Effort | Impact | Risk | Files |
|--------|--------|--------|------|-------|
| Categorical dtypes for CHROM, GENE, IMPACT, FILTER, EFFECT | Low | 70-97% memory reduction | Low | New utility + call sites |
| Pipe fusion for SnpSift filter + bgzip | Medium | Eliminates ~1GB temp file I/O | Medium | `filters.py`, `utils.py` |
| Vectorize comp_het result application | Medium | 100x+ faster Pass 2 | Medium | `analyzer.py`, `parallel_analyzer.py` |
| Replace `apply(axis=1)` in Pass 1 with itertuples | Low | ~10x faster deduction loop | Low | `analyzer.py:86`, `parallel_analyzer.py:145` |
| Numeric downcasting (int64→int32, float64→float32) | Low | 50-87% savings on numeric cols | Low | New utility function |

**Tier 3: Strategic (weeks+)**

| Change | Effort | Impact | Risk |
|--------|--------|--------|------|
| Full vectorization of `deduce_patterns_for_variant` | High | 100x+ for Pass 1 | High |
| Async subprocess for concurrent pipeline steps | Medium | I/O overlap | Medium |
| SharedMemory for parallel processing | High | Eliminate pickle serialization | High |

#### 3.4.4 Detail: PyArrow CSV Backend

The codebase has **27+ calls to `pd.read_csv()`**. PyArrow engine provides
multi-threaded parsing, 10-40x faster for larger files, ~50% less memory for
string-heavy data.

```python
# BEFORE
df = pd.read_csv(input_path, sep="\t", dtype=str, compression=compression)

# AFTER
df = pd.read_csv(input_path, sep="\t", dtype=str, compression=compression, engine="pyarrow")
```

For reads without `dtype=str`:
```python
df = pd.read_csv(tsv_file, sep="\t", engine="pyarrow", dtype_backend="pyarrow")
```

Gotcha: PyArrow engine is less robust for CSV edge cases. Test with real data.

Sources: [PyArrow CSV benchmark](https://pythonspeed.com/articles/pandas-read-csv-fast/),
[pandas PyArrow docs](https://pandas.pydata.org/docs/user_guide/pyarrow.html)

#### 3.4.5 Detail: Categorical dtypes

13+ `groupby("GENE")` operations across the codebase. Categorical dtype reduces
memory 70-97% and speeds groupby 10x when `observed=True` is set.

```python
def optimize_genomic_dtypes(df: pd.DataFrame) -> pd.DataFrame:
    categorical_columns = [
        "CHROM", "FILTER", "IMPACT", "EFFECT", "GENE",
        "Inheritance_Pattern", "BIOTYPE", "STRAND",
    ]
    for col in categorical_columns:
        if col in df.columns:
            df[col] = df[col].astype("category")
    return df
```

Critical: `groupby("GENE")` with categoricals can be **3500x SLOWER** without
`observed=True` (pandas < 3.0). Always use `observed=True, sort=False`.

#### 3.4.6 Detail: Subprocess Pipe Fusion

Current: `SnpSift filter` → temp.vcf → `bgzip` → output.vcf.gz → `bcftools index`

Fused: `SnpSift filter | bgzip → output.vcf.gz`, then `bcftools index`

```python
def apply_snpsift_filter_piped(variant_file, filter_string, cfg, output_file):
    snpsift_proc = subprocess.Popen(
        ["SnpSift", "filter", filter_string, variant_file],
        stdout=subprocess.PIPE, stderr=subprocess.PIPE,
    )
    with open(output_file, "wb") as out_f:
        bgzip_proc = subprocess.Popen(
            ["bgzip", "-@", str(cfg.get("threads", 1)), "-c"],
            stdin=snpsift_proc.stdout, stdout=out_f, stderr=subprocess.PIPE,
        )
    snpsift_proc.stdout.close()  # Allow SIGPIPE
    bgzip_proc.wait()
    snpsift_proc.wait()
    # ... check return codes, index result ...
```

Impact: eliminates temp file I/O. For 1 GB VCF, avoids writing/reading ~1 GB uncompressed.

#### 3.4.7 Detail: Reducing iterrows/apply

30+ `iterrows()` calls and 12+ `apply()` calls across critical paths.
Vectorization is 100-740x faster; itertuples is 10x faster as a minimal change.

```python
# BEFORE (inheritance Pass 2, analyzer.py:128)
for idx, row in df.iterrows():
    variant_key = create_variant_key(row)
    if gene in comp_het_results and variant_key in comp_het_results[gene]:
        df.at[idx, "_comp_het_info"] = comp_het_results[gene][variant_key]

# AFTER (vectorized)
df["_variant_key"] = df["CHROM"] + ":" + df["POS"] + ":" + df["REF"] + ":" + df["ALT"]
flat_lookup = {vk: info for g in comp_het_results.values() for vk, info in g.items()}
df["_comp_het_info"] = df["_variant_key"].map(flat_lookup)
df.drop(columns=["_variant_key"], inplace=True)
```

#### 3.4.8 Detail: SharedMemory Assessment

Current `parallel_analyzer.py` pickles gene DataFrames for ProcessPoolExecutor.

**Assessment:** Given that:
1. Gene-level DataFrames are small (few to few hundred rows)
2. Working ProcessPoolExecutor already exists
3. SharedMemory requires complex string column handling (segfaults with `dtype=object`)

**Recommendation: DEFER.** The complexity outweighs the benefit for current
gene-level parallelization. Pickle overhead for small groups is minimal.
Revisit only if profiling shows serialization as a bottleneck.

#### 3.4.9 I/O Optimization

- Replace deprecated `tempfile.mktemp()` with `NamedTemporaryFile(delete=False)` + `try/finally`
- Increase buffer sizes for gzip I/O
- Note: pipe fusion (3.4.6) eliminates the biggest temp file bottleneck

#### 3.4.10 What NOT to Do

- Do not migrate to Polars (clinical tool, deep pandas integration, unacceptable risk)
- Do not target free-threaded Python 3.13+ (numpy/pandas lack support)
- Do not pursue SIMD/cache-aware layouts (diminishing returns, high complexity)
- Do not optimize without benchmarks measuring the improvement
- Do not add SharedMemory complexity until profiling shows serialization as bottleneck

#### 3.4.11 Expected Impact Summary

| Optimization | Speedup | Memory Savings | Risk |
|-------------|---------|---------------|------|
| PyArrow CSV engine | 10-40x CSV reads | ~50% string columns | Low |
| Categorical dtypes | 10x groupby | 70-97% categorical cols | Low |
| itertuples over iterrows | ~10x iteration | None | Very Low |
| Vectorize comp_het assignment | ~100x | None | Medium |
| Pipe fusion (SnpSift+bgzip) | Eliminates temp I/O | Eliminates temp disk | Medium |
| gc.collect between stages | N/A | Potentially GBs freed | Very Low |
| Numeric downcasting | N/A | 50-87% numeric cols | Low |

#### 3.4.12 Key References

- [PyArrow CSV benchmark](https://pythonspeed.com/articles/pandas-read-csv-fast/)
- [pandas PyArrow docs](https://pandas.pydata.org/docs/user_guide/pyarrow.html)
- [pandas Categorical docs](https://pandas.pydata.org/docs/user_guide/categorical.html)
- [Categorical groupby 3500x slowdown](https://github.com/pandas-dev/pandas/issues/32976)
- [Vectorization vs iterrows: 700x](https://www.linkedin.com/pulse/tutorial-basic-vectorization-pandas-iterrows-apply-duc-lai-trung-minh-75d4c)
- [bcftools pipe streaming](https://samtools.github.io/bcftools/bcftools.html)
- [subprocess pipe chaining](https://rednafi.com/python/unix_style_pipeline_with_subprocess/)
- [Scalene profiler](https://github.com/plasma-umass/scalene)
- [py-spy profiler](https://github.com/benfred/py-spy)
- [pandas 3.0 Copy-on-Write](https://pandas.pydata.org/docs/whatsnew/v3.0.0.html)

---

## 4. Dependency Graph

```
#60 (test datasets) ──────────┐
                               ├──> #61 (report validation)
#58 (benchmarks) ─────────────┤
                               └──> #62 (performance optimization)
```

Recommended execution order: **#60 → #58 → #61 → #62**

#60 (datasets) provides the test data that #58 and #61 both need. #58 (benchmarks)
must exist before #62 (optimization) can be measured. #61 (report validation) is
independent but benefits from having real test data from #60.

---

## 5. Risk Assessment

| Risk | Severity | Mitigation |
|------|----------|------------|
| No correctness bugs | NONE | All known bugs resolved |
| Performance work without benchmarks | LOW | #58 before #62 |
| Real-world test data maintenance | LOW | Pinned URLs, checksums, lazy download |
| Large stage files hard to review | LOW | Accept; split only when adding features |
| mypy pandas type stubs | LOW | Not our code; suppress with `# type: ignore` |
| PyArrow engine edge cases | LOW | Test with real pipeline data before merging |
| Categorical groupby silent regression | LOW | Always use `observed=True` |

---

## 6. Summary

**Project health: Excellent.** All 30 historical issues resolved. CI pipeline
complete (lint + type check + tests + Docker). Modern tooling (hatchling, ruff,
mypy). 1035 tests passing, 0 failures.

Remaining work is all quality-of-life: performance benchmarks, real-world test
data, report validation, and optional performance optimization. None are urgent.
The recommended path is **#60 (datasets) → #58 (benchmarks) → #61 (reports) → #62
(optimization)**, with total estimated effort of 2-3 focused days.
