---
phase: 08-dataframe-optimization
plan: 04
type: execute
wave: 3
depends_on: ["08-02", "08-03"]
files_modified:
  - tests/performance/benchmark_dataframe_io.py
autonomous: true

must_haves:
  truths:
    - "Benchmarks measure PyArrow vs C engine CSV read speedup"
    - "Benchmarks measure memory reduction from categorical dtypes"
    - "Benchmarks measure itertuples vs iterrows speedup on actual analysis functions"
    - "Memory profiling shows 50-70% reduction on variant DataFrames"
    - "I/O benchmarks show 2-3x speedup from PyArrow engine"
  artifacts:
    - path: "tests/performance/benchmark_dataframe_io.py"
      provides: "Updated DataFrame I/O benchmarks with Phase 8 optimization measurements"
      contains: "benchmark"
    - path: ".planning/phases/08-dataframe-optimization/08-04-SUMMARY.md"
      provides: "Benchmark results and performance analysis"
  key_links:
    - from: "tests/performance/benchmark_dataframe_io.py"
      to: "variantcentrifuge/dataframe_optimizer.py"
      via: "import and benchmark load_optimized_dataframe"
      pattern: "load_optimized_dataframe"
---

<objective>
Run benchmarks to measure and verify the performance improvements from Phase 8 optimizations: PyArrow engine speedup, categorical dtype memory reduction, itertuples iteration speedup, and DataFrame pass-through I/O savings.

Purpose: Quantify the actual improvements to verify they meet the phase targets (50-70% memory reduction, 2-3x I/O speedup, 10-13x iteration speedup). Update benchmark baselines for Phase 9 comparison.

Output: Updated benchmark results, performance analysis in summary document.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-dataframe-optimization/08-01-SUMMARY.md
@.planning/phases/08-dataframe-optimization/08-02-SUMMARY.md
@.planning/phases/08-dataframe-optimization/08-03-SUMMARY.md
@tests/performance/benchmark_dataframe_io.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update DataFrame I/O benchmarks and run full benchmark suite</name>
  <files>
    tests/performance/benchmark_dataframe_io.py
  </files>
  <action>
**Update `tests/performance/benchmark_dataframe_io.py`** to include Phase 8 optimization benchmarks:

1. **PyArrow vs C engine comparison:**
   - Create a benchmark that reads a synthetic variant TSV (use existing synthetic data generators from Phase 6) with `engine="pyarrow"` vs default C engine
   - Use `benchmark()` fixture for timing
   - Measure at 1K, 10K scales

2. **Categorical dtype memory measurement:**
   - Load same synthetic TSV with and without categorical dtypes
   - Measure `df.memory_usage(deep=True).sum()` for both
   - Assert memory reduction >= 40% (conservative vs 50-70% target to avoid flaky tests)

3. **itertuples vs iterrows comparison:**
   - Benchmark a simple iteration loop (mimicking inheritance analysis pattern) with iterrows vs itertuples on 10K row DataFrame
   - Assert itertuples is at least 5x faster (conservative vs 10-14x expected)

4. **load_optimized_dataframe integration benchmark:**
   - Benchmark `load_optimized_dataframe` on synthetic variant TSV at 1K, 10K scales
   - Compare with plain `pd.read_csv(sep="\t", dtype=str)` baseline

**Then run the full benchmark suite:**
```bash
cd /mnt/c/development/scholl-lab/variantcentrifuge
python -m pytest tests/performance/ -v --timeout=600 -k "dataframe"
```

Also run the inheritance and gene burden benchmarks to check for regressions or improvements from itertuples:
```bash
python -m pytest tests/performance/ -v --timeout=600 -k "inheritance or gene_burden"
```

**Record results** in the summary with a table comparing Phase 7 vs Phase 8 baselines.

If any benchmark shows a regression (slower than Phase 7 baseline), investigate and document the cause. Acceptable if within 10% variance (benchmark noise).
  </action>
  <verify>
    `cd /mnt/c/development/scholl-lab/variantcentrifuge && python -m pytest tests/performance/benchmark_dataframe_io.py -v --timeout=600`
    `cd /mnt/c/development/scholl-lab/variantcentrifuge && python -m pytest tests/unit/ -v -x --timeout=60`
  </verify>
  <done>
    DataFrame I/O benchmarks updated with Phase 8 measurements. Memory reduction quantified (target: 50-70%). I/O speedup quantified (target: 2-3x). itertuples speedup quantified (target: 10-13x). All results documented in summary. No regressions in inheritance or gene burden benchmarks.
  </done>
</task>

</tasks>

<verification>
- `python -m pytest tests/performance/benchmark_dataframe_io.py -v` all pass
- `python -m pytest tests/performance/ -v -k "inheritance or gene_burden"` no regressions
- `python -m pytest tests/unit/ -v --timeout=60` all pass
- Summary document contains before/after performance table
</verification>

<success_criteria>
- Memory reduction measured and documented (target: 50-70% on variant DataFrames)
- I/O speedup measured and documented (target: 2-3x from PyArrow)
- Iteration speedup measured and documented (target: 10-13x from itertuples)
- No regressions in existing benchmarks
- Results table in summary for Phase 9 baseline comparison
</success_criteria>

<output>
After completion, create `.planning/phases/08-dataframe-optimization/08-04-SUMMARY.md`
</output>
