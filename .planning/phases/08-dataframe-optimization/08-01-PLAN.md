---
phase: 08-dataframe-optimization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - variantcentrifuge/dataframe_optimizer.py
  - variantcentrifuge/stages/analysis_stages.py
  - variantcentrifuge/pipeline_core/context.py
  - pyproject.toml
  - tests/unit/test_dataframe_optimizer.py
autonomous: true

must_haves:
  truths:
    - "PyArrow engine used for hot-path variant DataFrame loading in DataFrameLoadingStage"
    - "Low-cardinality columns auto-detected and loaded as categorical dtype"
    - "Columns with invalid Python identifiers renamed at load time for itertuples compatibility"
    - "PipelineContext has variants_df field for in-memory DataFrame pass-through"
    - "Memory threshold auto-fallback prevents OOM on 8-16GB desktops"
  artifacts:
    - path: "variantcentrifuge/dataframe_optimizer.py"
      provides: "DataFrame optimization utilities: PyArrow loading, categorical detection, column sanitization, memory threshold"
      exports: ["load_optimized_dataframe", "detect_categorical_columns", "rename_invalid_identifiers", "should_use_memory_passthrough"]
    - path: "tests/unit/test_dataframe_optimizer.py"
      provides: "Unit tests for all optimizer functions"
      min_lines: 80
  key_links:
    - from: "variantcentrifuge/stages/analysis_stages.py"
      to: "variantcentrifuge/dataframe_optimizer.py"
      via: "import and call in DataFrameLoadingStage._process"
      pattern: "load_optimized_dataframe"
    - from: "variantcentrifuge/pipeline_core/context.py"
      to: "variantcentrifuge/stages/analysis_stages.py"
      via: "variants_df field used for pass-through"
      pattern: "variants_df"
---

<objective>
Create the DataFrame optimization utility module and integrate it into the stage-based pipeline's DataFrameLoadingStage.

Purpose: This is the foundation for all Phase 8 optimizations. PyArrow engine provides 5-15x CSV read speedup, categorical dtypes provide 50-75% memory reduction on low-cardinality columns, and column sanitization enables safe itertuples usage in later plans.

Output: New `dataframe_optimizer.py` module with all optimization functions, integrated into DataFrameLoadingStage, with pyarrow as a required dependency.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-dataframe-optimization/08-CONTEXT.md
@.planning/phases/08-dataframe-optimization/08-RESEARCH.md
@variantcentrifuge/stages/analysis_stages.py
@variantcentrifuge/pipeline_core/context.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create dataframe_optimizer.py utility module and add pyarrow dependency</name>
  <files>
    variantcentrifuge/dataframe_optimizer.py
    pyproject.toml
    tests/unit/test_dataframe_optimizer.py
  </files>
  <action>
Create `variantcentrifuge/dataframe_optimizer.py` with the following functions:

1. `detect_categorical_columns(csv_path, sep="\t", cardinality_threshold=0.5, max_sample_rows=10000) -> dict[str, str]`
   - Sample first N rows with `pd.read_csv(..., nrows=max_sample_rows, dtype=str)`
   - For each column: if `nunique() / len(col) < cardinality_threshold`, map to `"category"`, else map to `"str"`
   - Return dtype dict suitable for `pd.read_csv(dtype=...)`
   - Log which columns were detected as categorical at DEBUG level

2. `rename_invalid_identifiers(df: pd.DataFrame) -> tuple[pd.DataFrame, dict[str, str]]`
   - Check each column name with `col.isidentifier()` and also check `keyword.iskeyword(col)`
   - For invalid names: `re.sub(r'[^a-zA-Z0-9_]', '_', col)`, prefix with `col_` if starts with digit
   - Handle duplicates by appending `_N` suffix
   - Return (renamed_df, rename_map) so downstream code can reverse if needed for output
   - Log renames at INFO level
   - Common renames: `GEN[0].GT` -> `GEN_0__GT`, `ANN[0].EFFECT` -> `ANN_0__EFFECT`

3. `should_use_memory_passthrough(df: pd.DataFrame, threshold_ratio=0.25) -> bool`
   - Use `psutil.virtual_memory().available` to get available RAM
   - Use `df.memory_usage(deep=True).sum()` for DataFrame size
   - Return True if df_memory <= available * threshold_ratio
   - Log decision at INFO level with sizes

4. `load_optimized_dataframe(file_path, sep="\t", compression=None, use_pyarrow=True, optimize_dtypes=True, sanitize_columns=True, **extra_kwargs) -> tuple[pd.DataFrame, dict[str, str]]`
   - Main entry point combining all optimizations
   - If `optimize_dtypes`: call `detect_categorical_columns` first, merge result into dtype dict
   - If `use_pyarrow` and no unsupported params (check for `on_bad_lines`, `converters`, `skipfooter`, `chunksize` in extra_kwargs): use `engine="pyarrow"`, else fallback to C engine with a DEBUG log
   - Always use `keep_default_na=False, na_values=[""], quoting=3` (matching current DataFrameLoadingStage behavior)
   - If `sanitize_columns`: call `rename_invalid_identifiers`
   - Return (df, column_rename_map)
   - Log memory usage before/after optimization at INFO level

5. `get_column_rename_map(context) -> dict[str, str]`
   - Helper to retrieve stored rename map from PipelineContext.stage_results

Add `"pyarrow>=14.0"` to the `dependencies` list in `pyproject.toml` (NOT in dev dependencies, it's a required runtime dep).

Create `tests/unit/test_dataframe_optimizer.py` with tests:
- Test `detect_categorical_columns` with a temp TSV: low-cardinality col detected, high-cardinality col stays str
- Test `rename_invalid_identifiers` with columns like `GEN[0].GT`, `ANN[0].EFFECT`, `CHROM` (valid, should not change), digit-start `0_foo`
- Test `should_use_memory_passthrough` returns bool (mock psutil)
- Test `load_optimized_dataframe` with a temp TSV: verifies PyArrow engine used, categoricals applied, columns sanitized
- Test PyArrow fallback when `on_bad_lines` param is passed
- Mark all tests with `@pytest.mark.unit`
  </action>
  <verify>
    `cd /mnt/c/development/scholl-lab/variantcentrifuge && python -m pytest tests/unit/test_dataframe_optimizer.py -v`
  </verify>
  <done>
    All optimizer unit tests pass. PyArrow is in pyproject.toml dependencies. Module exports all 5 functions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate optimizer into DataFrameLoadingStage and add variants_df to PipelineContext</name>
  <files>
    variantcentrifuge/pipeline_core/context.py
    variantcentrifuge/stages/analysis_stages.py
  </files>
  <action>
**In `variantcentrifuge/pipeline_core/context.py`:**
- Add `variants_df: pd.DataFrame | None = None` field after `current_dataframe` (line ~156). This is the optimized DataFrame with sanitized columns for in-memory pass-through to output stages.
- Add `column_rename_map: dict[str, str] = field(default_factory=dict)` field to store the rename mapping for output reversal.
- Update `merge_from` to handle `variants_df` and `column_rename_map` (same pattern as `current_dataframe`).

**In `variantcentrifuge/stages/analysis_stages.py` DataFrameLoadingStage (class at line 476):**

Modify the `_process` method's full DataFrame loading path (around lines 610-631) to use the optimizer:

1. Add import at top: `from ..dataframe_optimizer import load_optimized_dataframe, should_use_memory_passthrough`
2. Replace the `pd.read_csv(...)` call (lines 616-626) with:
   ```python
   df, rename_map = load_optimized_dataframe(
       str(input_file),
       sep="\t",
       compression=compression,
   )
   ```
3. Store rename map: `context.column_rename_map = rename_map`
4. After setting `context.current_dataframe = df`, also check pass-through:
   ```python
   if should_use_memory_passthrough(df):
       context.variants_df = df
       logger.info("DataFrame stored for in-memory pass-through")
   ```

Also update the checkpoint-skip path (lines 540-553) similarly, using `load_optimized_dataframe`.

Do NOT modify any other read_csv calls outside DataFrameLoadingStage (config reads, gene lists, BED files, etc. stay as-is per CONTEXT.md decision).
  </action>
  <verify>
    `cd /mnt/c/development/scholl-lab/variantcentrifuge && python -m pytest tests/unit/ -v -x --timeout=60`
    `cd /mnt/c/development/scholl-lab/variantcentrifuge && python -m pytest tests/integration/ -v -x --timeout=120 -k "not slow"`
  </verify>
  <done>
    DataFrameLoadingStage uses load_optimized_dataframe for both normal and checkpoint-skip paths. PipelineContext has variants_df and column_rename_map fields. All existing tests still pass with no behavioral changes.
  </done>
</task>

</tasks>

<verification>
- `python -c "from variantcentrifuge.dataframe_optimizer import load_optimized_dataframe, detect_categorical_columns, rename_invalid_identifiers, should_use_memory_passthrough; print('OK')"` succeeds
- `python -c "from variantcentrifuge.pipeline_core.context import PipelineContext; print(hasattr(PipelineContext, '__dataclass_fields__') and 'variants_df' in PipelineContext.__dataclass_fields__)"` prints True
- `python -m pytest tests/unit/test_dataframe_optimizer.py -v` all pass
- `python -m pytest tests/unit/ -v --timeout=60` no regressions
</verification>

<success_criteria>
- PyArrow engine is used for main variant DataFrame loading (with automatic C engine fallback for unsupported params)
- Low-cardinality columns auto-detected as categorical at load time
- Invalid column identifiers renamed at load time with mapping stored for output reversal
- PipelineContext.variants_df field available for downstream stages
- Memory pass-through auto-fallback at 25% available RAM threshold
- All existing tests pass unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/08-dataframe-optimization/08-01-SUMMARY.md`
</output>
