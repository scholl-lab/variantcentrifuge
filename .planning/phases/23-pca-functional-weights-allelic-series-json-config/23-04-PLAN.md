---
phase: 23-pca-functional-weights-allelic-series-json-config
plan: 04
type: execute
wave: 4
depends_on: ["23-03"]
files_modified:
  - variantcentrifuge/stages/analysis_stages.py
  - variantcentrifuge/association/diagnostics.py
  - variantcentrifuge/config.py
  - variantcentrifuge/cli.py
  - tests/unit/test_json_config.py
  - tests/unit/test_qq_plot.py
autonomous: true

must_haves:
  truths:
    - "User can add an 'association' section to config.json and have all fields applied"
    - "CLI flags override JSON config values"
    - "Invalid JSON config keys produce a clear error listing all unknown keys"
    - "JSON config is validated at startup before any processing"
    - "The 'association' key from config.json is forwarded through load_config() into context.config"
    - "When matplotlib is installed, --diagnostics-output produces a QQ plot PNG"
    - "When matplotlib is absent, pipeline completes without error and logs INFO about skipped plot"
  artifacts:
    - path: "variantcentrifuge/stages/analysis_stages.py"
      provides: "JSON config loading + validation in _build_assoc_config_from_context()"
      contains: "_build_assoc_config_from_context"
    - path: "variantcentrifuge/association/diagnostics.py"
      provides: "write_qq_plot() with lazy matplotlib import"
      exports: ["write_qq_plot"]
    - path: "tests/unit/test_json_config.py"
      provides: "Unit tests for JSON config loading, validation, CLI override"
      min_lines: 80
    - path: "tests/unit/test_qq_plot.py"
      provides: "Unit tests for QQ plot generation"
      min_lines: 40
  key_links:
    - from: "variantcentrifuge/config.py"
      to: "context.config"
      via: "load_config() returns full dict including 'association' key; setup_stages.py assigns context.config = config"
      pattern: "context\\.config = config|load_config"
    - from: "variantcentrifuge/stages/analysis_stages.py"
      to: "context.config (association section)"
      via: "_build_assoc_config_from_context reads context.config['association']"
      pattern: "context\\.config.*association"
    - from: "variantcentrifuge/association/diagnostics.py"
      to: "matplotlib"
      via: "lazy import with matplotlib.use('Agg') before pyplot"
      pattern: "matplotlib\\.use.*Agg"
---

<objective>
Implement JSON config mode (association section in existing config.json with CLI override precedence and fail-fast validation) and optional matplotlib QQ plot generation in diagnostics.

Purpose: Enable reproducible batch HPC workflows via JSON config and optional visual diagnostics for users with matplotlib installed.
Output: JSON config builder/validator in analysis_stages.py, write_qq_plot() in diagnostics.py, unit tests for both.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/23-pca-functional-weights-allelic-series-json-config/23-CONTEXT.md
@.planning/phases/23-pca-functional-weights-allelic-series-json-config/23-RESEARCH.md
@.planning/phases/23-pca-functional-weights-allelic-series-json-config/23-01-SUMMARY.md
@.planning/phases/23-pca-functional-weights-allelic-series-json-config/23-02-SUMMARY.md
@.planning/phases/23-pca-functional-weights-allelic-series-json-config/23-03-SUMMARY.md

Key source files:
@variantcentrifuge/stages/analysis_stages.py (AssociationAnalysisStage._process, AssociationConfig construction)
@variantcentrifuge/association/diagnostics.py (write_diagnostics, compute_qq_data)
@variantcentrifuge/association/base.py (AssociationConfig with all fields from Plans 01-03)
@variantcentrifuge/config.py (load_config function — returns full dict from JSON, including any 'association' key)
@variantcentrifuge/stages/setup_stages.py (ConfigurationLoadingStage: context.config = config at line ~206)
@variantcentrifuge/cli.py (config dict building section)
</context>

<tasks>

<task type="auto">
  <name>Task 1: JSON config loading, validation, CLI override, and config path verification</name>
  <files>
    variantcentrifuge/stages/analysis_stages.py
    variantcentrifuge/config.py
    variantcentrifuge/cli.py
    tests/unit/test_json_config.py
  </files>
  <action>
    **Step 0: Verify the config.json -> context.config path exists.**

    The existing pipeline already forwards config.json contents to context.config:
    - `variantcentrifuge/config.py` `load_config()` reads the JSON file and returns the full dict (all top-level keys).
    - `variantcentrifuge/stages/setup_stages.py` `ConfigurationLoadingStage._process()` calls `load_config()` and assigns `context.config = config` (line ~206).
    - Therefore, if config.json contains `"association": {...}`, it will be available as `context.config["association"]`.

    **Verify this is true** by checking:
    1. Read `config.py` load_config() — confirm it returns `json.load(f)` without filtering keys.
    2. Read `setup_stages.py` ConfigurationLoadingStage — confirm `context.config = config` (the full dict, no key filtering).
    3. If the path works as expected, no changes to config.py or cli.py are needed for the JSON->context.config forwarding. Document this in the SUMMARY.

    **However**, for the classic pipeline path (cli.py), check whether cli.py also builds context.config. If cli.py constructs the config dict manually (cherry-picking fields from args), then the "association" key from config.json may be LOST in the classic pipeline path. If so, add:
    ```python
    # In cli.py config dict building section, after loading config:
    # Preserve 'association' section from config.json for JSON config mode
    if "association" in cfg:
        config_dict["association"] = cfg["association"]
    ```
    where `cfg` is the result of `load_config(args.config)`.

    Add `variantcentrifuge/config.py` and `variantcentrifuge/cli.py` to files_modified since we may need to add the forwarding line.

    **Step 1: Create `_build_assoc_config_from_context(context)`** helper function in analysis_stages.py (above AssociationAnalysisStage class or as a module-level function):

    ```python
    # Valid keys for the "association" section in config.json
    VALID_ASSOCIATION_KEYS = frozenset({
        "correction_method", "gene_burden_mode", "trait_type",
        "variant_weights", "variant_weight_params", "skat_backend", "skat_method",
        "covariate_file", "covariate_columns", "categorical_covariates",
        "pca_file", "pca_tool", "pca_components",
        "coast_weights",
        "association_tests",
        "min_cases", "max_case_control_ratio", "min_case_carriers",
        "diagnostics_output",
        "confidence_interval_method", "confidence_interval_alpha",
        "continuity_correction", "missing_site_threshold", "missing_sample_threshold",
        "firth_max_iter",
    })

    def _validate_association_config_dict(d: dict) -> None:
        """Validate association config dict. Raises ValueError listing ALL invalid keys."""
        errors = []
        unknown = sorted(set(d) - VALID_ASSOCIATION_KEYS)
        if unknown:
            errors.append(f"Unknown keys: {unknown}")

        # Type validation for known keys
        str_keys = {"correction_method", "gene_burden_mode", "trait_type", "variant_weights",
                     "skat_backend", "skat_method", "covariate_file", "pca_file", "pca_tool",
                     "confidence_interval_method", "diagnostics_output"}
        int_keys = {"pca_components", "min_cases", "min_case_carriers", "firth_max_iter"}
        float_keys = {"max_case_control_ratio", "confidence_interval_alpha",
                       "continuity_correction", "missing_site_threshold", "missing_sample_threshold"}
        list_str_keys = {"covariate_columns", "categorical_covariates", "association_tests"}
        list_float_keys = {"coast_weights"}

        for key in str_keys & set(d):
            if d[key] is not None and not isinstance(d[key], str):
                errors.append(f"'{key}' must be a string, got {type(d[key]).__name__}")

        for key in int_keys & set(d):
            if not isinstance(d[key], int):
                errors.append(f"'{key}' must be an integer, got {type(d[key]).__name__}")

        for key in float_keys & set(d):
            if not isinstance(d[key], (int, float)):
                errors.append(f"'{key}' must be a number, got {type(d[key]).__name__}")

        for key in list_str_keys & set(d):
            if d[key] is not None and not isinstance(d[key], list):
                errors.append(f"'{key}' must be a list, got {type(d[key]).__name__}")

        for key in list_float_keys & set(d):
            if d[key] is not None and not isinstance(d[key], list):
                errors.append(f"'{key}' must be a list, got {type(d[key]).__name__}")

        # Validate enum-like values
        if "correction_method" in d and d["correction_method"] not in ("fdr", "bonferroni"):
            errors.append(f"'correction_method' must be 'fdr' or 'bonferroni', got '{d['correction_method']}'")
        if "trait_type" in d and d["trait_type"] not in ("binary", "quantitative"):
            errors.append(f"'trait_type' must be 'binary' or 'quantitative', got '{d['trait_type']}'")
        if "skat_backend" in d and d["skat_backend"] not in ("auto", "r", "python"):
            errors.append(f"'skat_backend' must be 'auto', 'r', or 'python', got '{d['skat_backend']}'")

        if errors:
            raise ValueError(
                f"Invalid 'association' config section ({len(errors)} error(s)):\n"
                + "\n".join(f"  - {e}" for e in errors)
            )
    ```

    **`_build_assoc_config_from_context(context)`**:
    - Read `json_assoc = context.config.get("association", {})` (from config.json loaded by -c/--config).
    - If non-empty, call `_validate_association_config_dict(json_assoc)` — fail fast with all errors.
    - For each AssociationConfig field, apply precedence: CLI value (from context.config direct keys) > JSON value (from json_assoc) > default.
    - Detect CLI-set values: a CLI arg is "set" if it differs from argparse default. The simplest approach: check context.config for the key; if present and not None, it was CLI-set. For flags with non-None defaults (like variant_weights="beta:1,25"), compare against the known default.
    - Return fully populated AssociationConfig.

    **Step 2: Replace inline AssociationConfig construction** in AssociationAnalysisStage._process():
    - Replace the existing ~20-line AssociationConfig(...) block (around line 2122) with:
      ```python
      assoc_config = _build_assoc_config_from_context(context)
      ```
    - This makes the config building testable independently.

    **Step 3: Unit tests** in tests/unit/test_json_config.py:
    - Test `_validate_association_config_dict()`:
      - Valid config dict -> no error
      - Unknown key -> ValueError listing the key
      - Multiple unknown keys -> all listed
      - Wrong type (int where str expected) -> type error
      - Invalid enum value (correction_method="invalid") -> error
      - Empty dict -> no error (all fields optional)
    - Test `_build_assoc_config_from_context()`:
      - JSON-only config: context has "association" section, no CLI overrides -> AssociationConfig fields match JSON
      - CLI override: context has "association" section AND direct CLI keys -> CLI wins
      - No association section: returns defaults
      - Mixed: some fields from JSON, some from CLI
    - **Test config.json -> context.config path**: Create a mock context with config dict containing an "association" key, call `_build_assoc_config_from_context(context)`, verify the association values are applied. This validates the full path from JSON to AssociationConfig.
    - Test full round-trip: create a mock context with config.json "association" section containing all fields, verify AssociationConfig matches expected values.
    - Mark all tests `@pytest.mark.unit`.
  </action>
  <verify>
    `pytest tests/unit/test_json_config.py -v` passes all tests.
    `make lint && make format` passes.
  </verify>
  <done>
    JSON config validation catches all invalid keys/types/values at startup with clear error messages.
    CLI values override JSON config values.
    AssociationConfig construction refactored into testable _build_assoc_config_from_context().
    Config.json 'association' key verified to flow through load_config() -> context.config -> _build_assoc_config_from_context().
    Unit tests verify validation, precedence, config path, and round-trip.
  </done>
</task>

<task type="auto">
  <name>Task 2: Matplotlib QQ plot in diagnostics.py and unit tests</name>
  <files>
    variantcentrifuge/association/diagnostics.py
    tests/unit/test_qq_plot.py
  </files>
  <action>
    1. **Add `write_qq_plot()` to diagnostics.py**:

    ```python
    def write_qq_plot(
        qq_data: pd.DataFrame,
        output_path: str | Path,
    ) -> bool:
        """
        Write QQ plot as PNG. Returns True if successful, False if matplotlib absent.

        Uses lazy import with matplotlib.use("Agg") for headless HPC environments.
        The Agg backend MUST be set before importing matplotlib.pyplot.

        Parameters
        ----------
        qq_data : pd.DataFrame
            QQ data with columns: test, expected_neg_log10_p, observed_neg_log10_p.
        output_path : str | Path
            Output file path (e.g. /path/to/qq_plot.png).

        Returns
        -------
        bool
            True if plot was written, False if matplotlib is not installed.
        """
        try:
            import matplotlib
            matplotlib.use("Agg")  # headless HPC — MUST be before pyplot import
            import matplotlib.pyplot as plt
        except ImportError:
            logger.info("matplotlib not installed — QQ plot skipped")
            return False

        if qq_data.empty:
            logger.info("No QQ data available — QQ plot skipped")
            return False

        fig, ax = plt.subplots(figsize=(6, 6))

        # Plot each test as separate series
        for test_name, group in qq_data.groupby("test"):
            ax.scatter(
                group["expected_neg_log10_p"],
                group["observed_neg_log10_p"],
                s=4, alpha=0.6, label=str(test_name),
            )

        # Identity line (expected = observed under null)
        max_val = max(
            qq_data["expected_neg_log10_p"].max(),
            qq_data["observed_neg_log10_p"].max(),
        ) + 0.5
        ax.plot([0, max_val], [0, max_val], "k--", linewidth=0.8, label="Expected")

        ax.set_xlabel("Expected -log10(p)")
        ax.set_ylabel("Observed -log10(p)")
        ax.set_title("QQ Plot")
        ax.legend(fontsize=8, loc="upper left")
        fig.tight_layout()

        plt.savefig(str(output_path), dpi=150, bbox_inches="tight")
        plt.close(fig)

        logger.info(f"QQ plot written to {output_path}")
        return True
    ```

    2. **Call write_qq_plot() from write_diagnostics()**:
    - After writing qq_data.tsv (around line 308), add:
      ```python
      # Optional QQ plot PNG (DIAG-04)
      qq_plot_path = diag_dir / "qq_plot.png"
      write_qq_plot(qq_combined, qq_plot_path)
      ```
    - This is a simple call — if matplotlib absent, it returns False and the pipeline continues.

    3. **Unit tests** in tests/unit/test_qq_plot.py:
    - Test `write_qq_plot()` with matplotlib available:
      - Create sample qq_data DataFrame with 2 test groups.
      - Write to tmp_path / "qq_plot.png".
      - Verify file exists and has non-zero size.
      - Verify function returns True.
    - Test `write_qq_plot()` with matplotlib mocked as absent:
      - Mock the import to raise ImportError.
      - Verify function returns False.
      - Verify log message contains "matplotlib not installed" (caplog).
    - Test `write_qq_plot()` with empty DataFrame:
      - Verify function returns False.
      - Verify log message about no QQ data.
    - Test that write_diagnostics() calls write_qq_plot() and produces qq_plot.png alongside qq_data.tsv (integration-style test using tmp_path).
    - Mark all tests `@pytest.mark.unit`.

    4. **Run full test suite** to verify no regressions:
    - `make test-fast`
    - `make ci-check`
  </action>
  <verify>
    `pytest tests/unit/test_qq_plot.py -v` passes all tests.
    `pytest tests/unit/test_json_config.py -v` passes (from Task 1).
    `make test-fast` passes (no regressions).
    `make ci-check` passes.
  </verify>
  <done>
    write_qq_plot() in diagnostics.py with lazy matplotlib import and Agg backend.
    write_diagnostics() produces qq_plot.png when matplotlib is available.
    Pipeline completes without error when matplotlib is absent (logs INFO).
    Unit tests cover matplotlib-present, matplotlib-absent, and empty-data cases.
    All existing tests pass; CI checks clean.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from variantcentrifuge.association.diagnostics import write_qq_plot; print('OK')"` imports cleanly
2. `pytest tests/unit/test_json_config.py tests/unit/test_qq_plot.py -v` — all tests pass
3. `make test-fast` — no regressions
4. `make ci-check` — full CI suite passes
</verification>

<success_criteria>
- JSON "association" section in config.json is validated at startup with fail-fast on invalid keys/types/values
- CLI flags override JSON config values (CLI > JSON > defaults)
- _build_assoc_config_from_context() produces correct AssociationConfig from mixed sources
- Config.json 'association' key is verified to flow through load_config() -> context.config without filtering
- write_qq_plot() produces qq_plot.png when matplotlib available
- Missing matplotlib: pipeline completes, single INFO log, no error
- write_diagnostics() automatically calls write_qq_plot() alongside existing TSV output
- All unit tests pass; full CI check clean
</success_criteria>

<output>
After completion, create `.planning/phases/23-pca-functional-weights-allelic-series-json-config/23-04-SUMMARY.md`
</output>
