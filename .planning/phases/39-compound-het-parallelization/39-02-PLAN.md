---
phase: 39-compound-het-parallelization
plan: 02
type: execute
wave: 2
depends_on: ["39-01"]
files_modified:
  - variantcentrifuge/inheritance/parallel_analyzer.py
  - variantcentrifuge/inheritance/comp_het_vectorized.py
autonomous: true

must_haves:
  truths:
    - "Workers in the parallel path receive only NumPy arrays — no DataFrame references in the hot path"
    - "Pedigree lookups in workers use pre-computed integer arrays, not dict.get() calls"
    - "All existing compound het tests pass without modification"
    - "Edge cases (single gene, 0 het variants, 1 CPU) work without error"
    - "VARIANTCENTRIFUGE_BATCH_SIZE environment variable controls batch size when set"
  artifacts:
    - path: "variantcentrifuge/inheritance/parallel_analyzer.py"
      provides: "Pre-dispatch dedup, pedigree arrays, numpy-only worker dispatch, batch size env var"
      contains: "build_pedigree_arrays"
    - path: "variantcentrifuge/inheritance/comp_het_vectorized.py"
      provides: "New numpy-only worker function for parallel path"
      contains: "_process_gene_group_arrays"
  key_links:
    - from: "variantcentrifuge/inheritance/parallel_analyzer.py"
      to: "variantcentrifuge/inheritance/comp_het_vectorized.py"
      via: "import _process_gene_group_arrays"
      pattern: "from .comp_het_vectorized import.*_process_gene_group_arrays"
    - from: "variantcentrifuge/inheritance/parallel_analyzer.py"
      to: "os.environ"
      via: "VARIANTCENTRIFUGE_BATCH_SIZE env var"
      pattern: "VARIANTCENTRIFUGE_BATCH_SIZE"
---

<objective>
Optimize compound het Pass 2 to eliminate GIL contention: move DataFrame operations to pre-dispatch, pre-compute pedigree arrays, create a numpy-only worker function, and add VARIANTCENTRIFUGE_BATCH_SIZE env var.

Purpose: This is the core optimization that delivers measurable speedup on multi-core machines (PERF-01, PERF-02). After this plan, workers operate almost entirely on GIL-releasing NumPy operations.

Output: Modified `parallel_analyzer.py` and `comp_het_vectorized.py` with the optimized parallel path. Sequential path unchanged.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/39-compound-het-parallelization/39-CONTEXT.md
@.planning/phases/39-compound-het-parallelization/39-RESEARCH.md
@.planning/phases/39-compound-het-parallelization/39-01-SUMMARY.md
@variantcentrifuge/inheritance/parallel_analyzer.py
@variantcentrifuge/inheritance/comp_het_vectorized.py
@variantcentrifuge/inheritance/analyzer.py
@variantcentrifuge/ped_reader.py
@variantcentrifuge/memory/resource_manager.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add pedigree array builder and numpy-only worker function</name>
  <files>variantcentrifuge/inheritance/parallel_analyzer.py, variantcentrifuge/inheritance/comp_het_vectorized.py</files>
  <action>
**In `parallel_analyzer.py`, add `build_pedigree_arrays()` function:**

```python
def build_pedigree_arrays(
    sample_list: list[str],
    pedigree_data: dict[str, dict[str, Any]],
    sample_to_idx: dict[str, int],
) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
```

- `father_idx_arr`: np.int32, index of father in sample_list (-1 if none/missing)
- `mother_idx_arr`: np.int32, index of mother in sample_list (-1 if none/missing)
- `affected_arr`: np.int8, 1 if affected (status "2"), else 0
- Use `sample_to_idx` for lookups (already exists from Pass 1)
- Father/mother IDs come from `pedigree_data[sample_id]["father_id"]` / `"mother_id"`, skip if "0" or not in sample_to_idx

**In `parallel_analyzer.py`, add `_get_batch_size()` function:**

```python
def _get_batch_size(n_workers: int) -> int:
```

- Check `os.environ.get("VARIANTCENTRIFUGE_BATCH_SIZE")`
- If set and valid integer >= 1: return it
- If set but invalid: `logger.warning(...)` and fall back to auto
- Auto: `min(4 * n_workers, 500)` (current behavior)
- Add `import os` at top of file

**In `comp_het_vectorized.py`, add `_process_gene_group_arrays()` function:**

Create a NEW function that is the numpy-only equivalent of the worker path. This function:
- Receives: `gene: str`, `n_unique: int`, `gene_row_indices: np.ndarray` (deduplicated), `variant_keys: np.ndarray` (deduplicated), `gt_matrix: np.ndarray`, `sample_to_idx: dict[str, int]`, `father_idx_arr: np.ndarray`, `mother_idx_arr: np.ndarray`, `affected_arr: np.ndarray`, `sample_list: list[str]`
- Returns: `tuple[str, dict[str, Any]]` (gene name, comp_het results dict)
- Does NOT call `drop_duplicates()`, `duplicated()`, or `iloc[]` — data is already deduplicated
- Does NOT call `get_parents()` or `is_affected()` from ped_reader — uses array lookups instead
- For each sample, parent lookup: `father_idx = father_idx_arr[sample_idx]`, if -1 then no father. Get father genotypes from gt_matrix column at that index.
- For affected check: `affected_arr[sample_idx] == 1`
- The core logic (het detection, find_potential_partners_vectorized, result dict building) stays the same as in `analyze_gene_for_compound_het_vectorized` but uses array-based pedigree lookups
- Import `find_potential_partners_vectorized` from within the same module (it's already there)

CRITICAL: Do NOT modify `analyze_gene_for_compound_het_vectorized()` — it must remain unchanged for the sequential path and for `analyzer.py`. The new function is a parallel-only worker.

CRITICAL: Do NOT modify `_process_gene_group()` — keep it for backward compatibility. The new dispatch code will call `_process_gene_group_arrays` directly.
  </action>
  <verify>
```bash
cd /mnt/c/development/scholl-lab/variantcentrifuge
python -c "from variantcentrifuge.inheritance.parallel_analyzer import build_pedigree_arrays, _get_batch_size; print('imports OK')"
python -c "from variantcentrifuge.inheritance.comp_het_vectorized import _process_gene_group_arrays; print('imports OK')"
make lint
```
  </verify>
  <done>New functions exist and import cleanly. Lint passes. Existing functions unchanged.</done>
</task>

<task type="auto">
  <name>Task 2: Wire pre-dispatch dedup and numpy-only workers into analyze_inheritance_parallel</name>
  <files>variantcentrifuge/inheritance/parallel_analyzer.py</files>
  <action>
Modify the parallel branch of `analyze_inheritance_parallel()` (the `if use_parallel and "GENE" in df.columns:` block, lines ~202-270) to:

1. **Pre-dispatch deduplication** — In the gene grouping loop (currently lines 206-211), add dedup in main thread:
   - Compute `dedup_mask = ~group_df.duplicated(subset=["CHROM", "POS", "REF", "ALT"])` in the loop
   - Count `n_unique = int(dedup_mask.sum())`; skip gene if n_unique < 2
   - Compute deduplicated row indices: `dedup_iloc = gene_iloc[dedup_mask.values]`
   - Compute deduplicated variant keys: `gene_vkeys_deduped = all_variant_keys[dedup_iloc]`
   - Compute deduplicated row positions: `gene_row_idx_deduped = row_positions[dedup_iloc]`
   - Append `(gene_name_str, n_unique, gene_row_idx_deduped, gene_vkeys_deduped)` — NO DataFrame reference

2. **Build pedigree arrays** — After the gene grouping loop, before the ThreadPoolExecutor block:
   ```python
   father_idx_arr, mother_idx_arr, affected_arr = build_pedigree_arrays(
       sample_list, pedigree_data, sample_to_idx
   )
   ```

3. **Use `_get_batch_size()`** — Replace `batch_size = min(4 * n_workers, 500)` with `batch_size = _get_batch_size(n_workers)`

4. **Dispatch to `_process_gene_group_arrays`** — Replace the `executor.submit(_process_gene_group, ...)` call with:
   ```python
   executor.submit(
       _process_gene_group_arrays,
       gene_name,
       n_unique,
       gene_row_idx,
       gene_vkeys,
       gt_matrix,
       sample_to_idx,
       father_idx_arr,
       mother_idx_arr,
       affected_arr,
       sample_list,
   )
   ```
   Update the batch unpacking to match the new tuple format: `for gene_name, n_unique, gene_row_idx, gene_vkeys in batch`

5. **Update import** — Add `_process_gene_group_arrays` to the import from `.comp_het_vectorized`

6. **Sort genes by n_unique descending** (load balancing) — Replace `key=lambda x: len(x[1])` with `key=lambda x: x[1]` since x[1] is now n_unique (int), not gene_df

7. **Leave the sequential branch (else block, lines ~271-298) completely unchanged** — it still uses `analyze_gene_for_compound_het_vectorized` with DataFrame.

8. **Verify no gene_df reference in the parallel dispatch** — After changes, `gene_df` / `group_df` should NOT appear in the ThreadPoolExecutor submit calls or in the `genes_with_multiple_variants` / `genes_to_dispatch` list. The main thread loop may still reference `group_df` for dedup computation (that's fine — it's the main thread).

After all changes, run:
```bash
grep "gene_df" variantcentrifuge/inheritance/parallel_analyzer.py
```
`gene_df` should only appear in: the `_process_gene_group` function (kept for backward compat) and the sequential branch. NOT in the parallel dispatch section.
  </action>
  <verify>
```bash
cd /mnt/c/development/scholl-lab/variantcentrifuge
# Lint
make lint

# All compound het tests must pass unchanged
pytest tests/test_inheritance/ -m comp_het -v --tb=short

# Full test suite
pytest -m "not slow" --tb=short -q

# Verify no DataFrame in parallel workers
grep -n "gene_df" variantcentrifuge/inheritance/parallel_analyzer.py
# Should only appear in _process_gene_group and sequential branch

# Verify env var works
VARIANTCENTRIFUGE_BATCH_SIZE=10 python -c "
from variantcentrifuge.inheritance.parallel_analyzer import _get_batch_size
assert _get_batch_size(4) == 10, 'env var not respected'
print('batch size env var OK')
"
```
  </verify>
  <done>
- Parallel path dispatches numpy-only workers (no DataFrame in hot path)
- Pedigree arrays pre-computed once in main thread
- VARIANTCENTRIFUGE_BATCH_SIZE env var controls batch size
- All existing tests pass without modification
- Sequential path unchanged
- `make ci-check` passes
  </done>
</task>

<task type="auto">
  <name>Task 3: Run post-optimization benchmark, compare with baseline, update roadmap</name>
  <files></files>
  <action>
1. **Run the benchmark again** and save output:
   ```bash
   python tests/performance/standalone_bench_comp_het_parallel.py | tee .planning/phases/39-compound-het-parallelization/39-AFTER.txt
   ```

2. **Compare with baseline** — Read both `39-BASELINE.txt` and `39-AFTER.txt`. Note whether parallel configurations show improvement over baseline. Log the comparison in the SUMMARY.

3. **Run `make ci-check`** to confirm everything is clean.

4. **Update ROADMAP.md** — Change Success Criterion 1 from:
   > `parallel_analyzer.py` uses ProcessPoolExecutor (or Numba) for compound het Pass 2 instead of ThreadPoolExecutor; the change is visible in the source.

   To:
   > `parallel_analyzer.py` eliminates GIL contention in compound het Pass 2 workers by pre-dispatching DataFrame operations and pre-computing pedigree arrays as NumPy integer arrays; workers receive only NumPy arrays in the hot path.

   This reflects the CONTEXT.md decision to keep ThreadPoolExecutor but eliminate GIL contention.

5. **Update ROADMAP.md progress table** — Set Phase 39 plans to the correct count and mark as complete.
  </action>
  <verify>
```bash
cd /mnt/c/development/scholl-lab/variantcentrifuge
make ci-check
# Compare files exist
cat .planning/phases/39-compound-het-parallelization/39-BASELINE.txt
cat .planning/phases/39-compound-het-parallelization/39-AFTER.txt
# Roadmap updated
grep "eliminates GIL contention" .planning/ROADMAP.md
```
  </verify>
  <done>
- Post-optimization benchmark captured in 39-AFTER.txt
- Baseline vs after comparison documented
- Roadmap success criteria updated to reflect ThreadPoolExecutor optimization
- `make ci-check` passes
  </done>
</task>

</tasks>

<verification>
- `make ci-check` passes
- `pytest tests/test_inheritance/ -m comp_het -v` — all pass without modification
- `pytest -m "not slow"` — full non-slow suite passes
- `grep "gene_df" variantcentrifuge/inheritance/parallel_analyzer.py` — not in parallel dispatch
- `grep "build_pedigree_arrays" variantcentrifuge/inheritance/parallel_analyzer.py` — exists
- `grep "_process_gene_group_arrays" variantcentrifuge/inheritance/comp_het_vectorized.py` — exists
- `grep "VARIANTCENTRIFUGE_BATCH_SIZE" variantcentrifuge/inheritance/parallel_analyzer.py` — exists
- Benchmark timing comparison shows parallel path timing (speedup expected but not asserted)
</verification>

<success_criteria>
1. Workers in the parallel path receive only NumPy arrays — no DataFrame in hot path
2. Pedigree lookups use pre-computed integer arrays (father_idx_arr, mother_idx_arr, affected_arr)
3. VARIANTCENTRIFUGE_BATCH_SIZE env var is respected
4. All existing compound het tests pass without modification
5. Edge cases (1 gene, 0 het variants, 1 worker) work without error
6. Benchmark comparison (baseline vs after) is captured
7. Roadmap success criteria updated to reflect actual approach
</success_criteria>

<output>
After completion, create `.planning/phases/39-compound-het-parallelization/39-02-SUMMARY.md`
</output>
