---
phase: 09-inheritance-analysis-optimization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/validate_inheritance.py
  - tests/test_inheritance/test_golden_files.py
  - tests/fixtures/golden/README.md
autonomous: true

must_haves:
  truths:
    - "Golden reference outputs can be generated from current (pre-vectorization) code"
    - "Validation script detects any difference in inheritance patterns between old and new output"
    - "Golden files cover trio, single-sample, no-pedigree, X-linked, and mitochondrial cases"
  artifacts:
    - path: "scripts/validate_inheritance.py"
      provides: "CLI script to generate golden files and compare outputs"
      exports: ["generate_golden", "compare_outputs"]
    - path: "tests/test_inheritance/test_golden_files.py"
      provides: "Pytest-based golden file comparison"
      min_lines: 50
    - path: "tests/fixtures/golden/README.md"
      provides: "Documentation of golden file format and generation"
  key_links:
    - from: "scripts/validate_inheritance.py"
      to: "variantcentrifuge/inheritance/analyzer.py"
      via: "imports analyze_inheritance"
      pattern: "from variantcentrifuge.inheritance.analyzer import"
    - from: "tests/test_inheritance/test_golden_files.py"
      to: "scripts/validate_inheritance.py"
      via: "imports validation functions"
      pattern: "validate_inheritance"
---

<objective>
Create golden file validation infrastructure for inheritance analysis vectorization.

Purpose: Before changing any inheritance logic, we need a way to prove the vectorized code produces clinically equivalent output. This plan generates reference outputs from the current (known-correct) implementation and builds a comparison script that will be used after each vectorization pass.

Output: A validation script, golden reference files, and a pytest test that runs golden file comparison.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-inheritance-analysis-optimization/09-CONTEXT.md
@.planning/phases/09-inheritance-analysis-optimization/09-RESEARCH.md
@variantcentrifuge/inheritance/analyzer.py
@variantcentrifuge/inheritance/deducer.py
@variantcentrifuge/inheritance/comp_het.py
@variantcentrifuge/inheritance/comp_het_vectorized.py
@variantcentrifuge/inheritance/prioritizer.py
@variantcentrifuge/inheritance/segregation_checker.py
@tests/test_inheritance/conftest.py
@tests/performance/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create golden file generation and comparison script</name>
  <files>scripts/validate_inheritance.py, tests/fixtures/golden/README.md</files>
  <action>
Create `scripts/validate_inheritance.py` with two modes:

**generate mode:** Creates golden reference files from the current implementation.
- Build synthetic DataFrames covering key scenarios:
  1. **Trio family** (proband + father + mother): de novo (child het, parents ref), autosomal dominant (child het, one parent het+affected), autosomal recessive (child hom_alt, both parents het), de_novo_candidate (one parent missing GT)
  2. **Single sample** (no pedigree): het variant -> "unknown", hom_alt -> "homozygous"
  3. **Extended family** (4+ members): dominant segregation across 3 generations
  4. **X-linked variants**: CHROM=X, male proband het (hemizygous), mother carrier
  5. **Mitochondrial variants**: CHROM=MT, maternal transmission
  6. **Compound het**: Two het variants in same gene, one from each parent (trans config)
  7. **Edge cases**: Missing genotypes ("./." for parents), empty pedigree dict, single variant per gene (no comp het)
- For each scenario:
  - Create DataFrame with columns: CHROM, POS, REF, ALT, GENE, sample GT columns
  - Create appropriate pedigree_data dict
  - Run `analyze_inheritance(df, pedigree_data, sample_list)`
  - Save result DataFrame to `tests/fixtures/golden/{scenario_name}.parquet` (preserves dtypes exactly)
  - Also save a human-readable `{scenario_name}.tsv` for inspection

**compare mode:** Compares new output against golden files.
- Load golden file, run analyze_inheritance with current code, compare:
  - Sort both DataFrames by CHROM, POS, REF, ALT (stable key per 09-CONTEXT.md)
  - Compare Inheritance_Pattern column: must match exactly
  - Compare Inheritance_Details column: parse JSON, compare primary_pattern, all_patterns, confidence (within epsilon=0.001), samples_with_pattern
  - Report: number of matches, mismatches, and specific diff details
  - Return exit code 0 if all match, 1 if any differ

Use argparse CLI: `python scripts/validate_inheritance.py generate` and `python scripts/validate_inheritance.py compare`

Important: Use the SAME synthetic data generation for both generate and compare (deterministic). Factor out scenario builders into functions that return (df, pedigree_data, sample_list, scenario_name).

Create `tests/fixtures/golden/README.md` explaining the golden file format and how to regenerate.
  </action>
  <verify>
    Run: `python scripts/validate_inheritance.py generate` -- should create .parquet and .tsv files in tests/fixtures/golden/
    Run: `python scripts/validate_inheritance.py compare` -- should report all scenarios pass (0 exit code)
    Run: `ls tests/fixtures/golden/*.parquet | wc -l` -- should show 7+ golden files
  </verify>
  <done>Golden files generated for all 7+ scenarios, comparison script passes with exit code 0</done>
</task>

<task type="auto">
  <name>Task 2: Create pytest golden file test</name>
  <files>tests/test_inheritance/test_golden_files.py</files>
  <action>
Create `tests/test_inheritance/test_golden_files.py` that:

1. Imports the scenario builder functions from `scripts/validate_inheritance.py` (add scripts to sys.path or use importlib)
2. For each scenario, parametrize a test:
   ```python
   @pytest.mark.parametrize("scenario_name", [list of scenario names])
   def test_golden_file_match(scenario_name):
   ```
3. Each test:
   - Builds the scenario (df, pedigree_data, sample_list)
   - Runs analyze_inheritance()
   - Loads the golden .parquet file
   - Sorts both by stable key (CHROM, POS, REF, ALT)
   - Asserts Inheritance_Pattern matches exactly
   - Parses Inheritance_Details JSON and compares primary_pattern, all_patterns, confidence
4. Mark tests with `@pytest.mark.unit` and `@pytest.mark.inheritance`
5. Add a fixture that skips if golden files don't exist (with clear message to run generate first)

Also add a standalone test `test_scenario_determinism` that runs each scenario twice and asserts identical output, proving the synthetic data is deterministic.
  </action>
  <verify>
    Run: `pytest tests/test_inheritance/test_golden_files.py -v` -- all tests pass
    Run: `pytest tests/test_inheritance/test_golden_files.py -v -k determinism` -- determinism test passes
  </verify>
  <done>All golden file tests pass, determinism verified, tests are marked with unit and inheritance markers</done>
</task>

</tasks>

<verification>
1. `python scripts/validate_inheritance.py generate` creates golden files without errors
2. `python scripts/validate_inheritance.py compare` exits with code 0
3. `pytest tests/test_inheritance/test_golden_files.py -v` -- all tests pass
4. `pytest -m unit` -- no regressions in existing tests
5. Golden files exist for trio, single-sample, extended-family, x-linked, mitochondrial, compound-het, and edge-case scenarios
</verification>

<success_criteria>
- Golden reference files generated from current (pre-vectorization) code
- Validation script can detect any difference in inheritance patterns
- All golden file pytest tests pass
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/09-inheritance-analysis-optimization/09-01-SUMMARY.md`
</output>
