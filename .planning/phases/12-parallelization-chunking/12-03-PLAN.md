---
phase: 12-parallelization-chunking
plan: 03
type: execute
wave: 2
depends_on: ["12-01"]
files_modified:
  - variantcentrifuge/pipeline_core/runner.py
  - tests/unit/pipeline_core/test_runner_memory.py
autonomous: true

must_haves:
  truths:
    - "Pipeline runner reports per-stage memory usage (peak RSS delta) at INFO level after pipeline completes"
    - "Memory reporting uses psutil RSS tracking (not tracemalloc) for production accuracy"
    - "Memory summary includes total peak RSS and per-stage breakdown sorted by memory impact"
    - "Memory tracking adds negligible overhead (<1% wall time increase)"
  artifacts:
    - path: "variantcentrifuge/pipeline_core/runner.py"
      provides: "Stage execution with memory tracking and summary reporting"
      contains: "memory"
    - path: "tests/unit/pipeline_core/test_runner_memory.py"
      provides: "Tests for memory reporting in pipeline runner"
      min_lines: 40
  key_links:
    - from: "variantcentrifuge/pipeline_core/runner.py"
      to: "psutil"
      via: "RSS memory tracking"
      pattern: "psutil\\.Process.*memory_info.*rss"
---

<objective>
Add per-stage memory usage reporting to the pipeline runner so users can see which stages consume the most memory. Report at INFO level after pipeline completes.

Purpose: CONTEXT requires "Report memory usage statistics (total peak, per-stage breakdown) at INFO level after pipeline completes." This gives users visibility into memory consumption without requiring debug-level logging.

Output: Pipeline execution summary includes memory breakdown per stage.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-parallelization-chunking/12-CONTEXT.md
@variantcentrifuge/pipeline_core/runner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add per-stage memory tracking to PipelineRunner</name>
  <files>
    variantcentrifuge/pipeline_core/runner.py
  </files>
  <action>
**Enhance `_execute_stage()` method:**

Currently, memory logging only happens at DEBUG level. Promote to tracked metrics:

1. In `_execute_stage()`, ALWAYS track RSS before and after (not just at DEBUG):
   ```python
   process = psutil.Process()
   mem_before_mb = process.memory_info().rss / 1024 / 1024
   # ... execute stage ...
   mem_after_mb = process.memory_info().rss / 1024 / 1024
   ```

2. Store memory metrics in `self._stage_metrics` dict (already exists but unused):
   ```python
   self._stage_metrics[stage.name] = {
       "mem_before_mb": mem_before_mb,
       "mem_after_mb": mem_after_mb,
       "mem_delta_mb": mem_after_mb - mem_before_mb,
       "mem_peak_mb": max(mem_before_mb, mem_after_mb),
   }
   ```

3. Keep the existing DEBUG-level detailed logging as-is (backwards compatible).

**Enhance `_log_execution_summary()` method:**

After the existing timing summary, add a memory summary section:

```
============================================================
Memory Usage Summary
============================================================
Stage                          Before    After    Delta
inheritance_analysis           512.3 MB  1024.1 MB  +511.8 MB
dataframe_loading              256.1 MB  512.3 MB   +256.2 MB
excel_report                   1024.1 MB 890.2 MB   -133.9 MB
...
------------------------------------------------------------
Peak RSS: 1024.1 MB
============================================================
```

- Sort stages by absolute `mem_delta_mb` descending (biggest memory consumers first)
- Log at INFO level (not DEBUG — this is the CONTEXT requirement)
- Calculate peak RSS as max of all `mem_after_mb` values
- Only include the memory section if `self._stage_metrics` is non-empty (defensive)

**Enhance `run()` method:**

After `self._log_execution_summary()`, add a one-line INFO summary:
```python
if self._stage_metrics:
    peak_mb = max(m["mem_after_mb"] for m in self._stage_metrics.values())
    logger.info(f"Pipeline peak memory: {peak_mb:.0f} MB")
```

**Important:** The memory tracking uses `psutil.Process().memory_info().rss` which is the resident set size — actual physical memory used. This is more accurate than tracemalloc for production use (tracemalloc only tracks Python allocations, not C extensions like numpy/pandas).

**Performance consideration:** `psutil.Process().memory_info()` is a single syscall (~0.1ms). Two calls per stage adds negligible overhead even for 40+ stages.
  </action>
  <verify>
    `make lint` passes.
    `pytest tests/unit/pipeline_core/ -v --tb=short` passes (existing runner tests not broken).
  </verify>
  <done>
    Pipeline runner tracks per-stage memory usage and reports summary at INFO level after completion. Peak RSS reported. Stages sorted by memory impact.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add memory reporting tests</name>
  <files>
    tests/unit/pipeline_core/test_runner_memory.py
  </files>
  <action>
Create `tests/unit/pipeline_core/test_runner_memory.py` with tests for the memory reporting feature.

**Test approach:** Create minimal mock stages that allocate/free memory, run through PipelineRunner, verify metrics are captured.

Test cases:
1. `test_stage_metrics_captured`: Run 2 mock stages through runner. Verify `runner._stage_metrics` has entries for both stages with keys `mem_before_mb`, `mem_after_mb`, `mem_delta_mb`.

2. `test_memory_summary_logged`: Run pipeline, capture log output (use `caplog` fixture at INFO level). Verify "Memory Usage Summary" appears in log output. Verify "Peak RSS" appears in log output.

3. `test_metrics_empty_when_no_stages`: Create runner, call `_log_execution_summary()` with empty metrics. Verify no crash, no memory section logged.

**Mock stage pattern** (reuse from existing runner tests if available):
```python
class MockStage(Stage):
    @property
    def name(self): return "mock_stage"
    @property
    def dependencies(self): return set()
    def _process(self, context):
        return context
```

Use `@pytest.mark.unit` marker on all tests.

Check existing test patterns in `tests/unit/pipeline_core/` and follow the same conventions for imports, fixtures, and mock stage creation.
  </action>
  <verify>
    `pytest tests/unit/pipeline_core/test_runner_memory.py -v` passes all tests.
    `make ci-check` passes.
  </verify>
  <done>
    3 tests verify memory reporting: metrics captured, summary logged at INFO, handles empty gracefully. CI passes.
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/unit/pipeline_core/test_runner_memory.py -v` — all memory reporting tests pass
2. `pytest tests/unit/pipeline_core/ -v` — existing runner tests still pass
3. `make ci-check` — full CI passes
</verification>

<success_criteria>
- Per-stage memory tracking reports at INFO level (not just DEBUG)
- Memory summary sorted by impact, includes peak RSS
- Negligible overhead from psutil RSS calls
- 3+ tests verify the feature
- No regressions in existing runner tests
</success_criteria>

<output>
After completion, create `.planning/phases/12-parallelization-chunking/12-03-SUMMARY.md`
</output>
