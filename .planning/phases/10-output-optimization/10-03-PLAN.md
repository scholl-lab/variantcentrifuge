---
phase: 10-output-optimization
plan: 03
type: execute
wave: 2
depends_on: ["10-01", "10-02"]
files_modified:
  - tests/performance/benchmark_excel_output.py
  - tests/unit/test_excel_full_fidelity.py
autonomous: true

must_haves:
  truths:
    - "Benchmarks measure Excel generation time at 100, 1K, 10K, 50K variant scales"
    - "Benchmarks show 2-5x speedup for xlsxwriter vs openpyxl on initial write"
    - "Full fidelity test generates Excel with all 4 sheets, verifies hyperlinks, freeze panes, auto-filters"
    - "GT pre-parsing benchmark shows elimination of redundant regex parsing"
  artifacts:
    - path: "tests/performance/benchmark_excel_output.py"
      provides: "Excel generation benchmarks at multiple scales"
      min_lines: 80
    - path: "tests/unit/test_excel_full_fidelity.py"
      provides: "End-to-end Excel output fidelity test with all sheets and formatting"
      min_lines: 60
  key_links:
    - from: "tests/performance/benchmark_excel_output.py"
      to: "variantcentrifuge/converter.py"
      via: "benchmark calls convert_to_excel + finalize_excel_file"
      pattern: "convert_to_excel|finalize_excel_file"
---

<objective>
Benchmark Excel generation speedup and create comprehensive fidelity tests proving output equivalence.

Purpose: Phase 10 success criteria require benchmarks showing 2-5x Excel generation speedup and tests verifying hyperlinks, freeze panes, and auto-filters in output. This plan creates the evidence.

Output: Performance benchmarks at 4 scales, full fidelity end-to-end test, benchmark results saved for cross-phase comparison.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-output-optimization/10-CONTEXT.md

Prior plan outputs:
@.planning/phases/10-output-optimization/10-01-SUMMARY.md
@.planning/phases/10-output-optimization/10-02-SUMMARY.md

Key reference files:
@tests/performance/conftest.py
@tests/performance/benchmark_dataframe_io.py (for benchmark patterns)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Excel generation benchmarks</name>
  <files>
    tests/performance/benchmark_excel_output.py
  </files>
  <action>
Create `tests/performance/benchmark_excel_output.py` following existing benchmark patterns in the performance test suite.

1. **Setup helpers:**
   - `_create_variant_df(n_variants, n_samples=3)`: Generate synthetic DataFrame with columns matching variantcentrifuge output: CHROM, POS, REF, ALT, GT (formatted as "Sample1(0/1);Sample2(1/1)"), GENE, IMPACT, EFFECT, plus a few URL columns (e.g., SpliceAI_URL, gnomAD_URL containing "https://..." strings).
   - Use numpy random with fixed seed for reproducibility.
   - `_create_config()`: Return minimal config dict with `links: {}`, `igv_enabled: False`.

2. **Benchmarks (use pytest-benchmark via `benchmark` fixture):**

   a. `test_benchmark_excel_write_100(benchmark, tmp_path)` - 100 variants
   b. `test_benchmark_excel_write_1k(benchmark, tmp_path)` - 1,000 variants
   c. `test_benchmark_excel_write_10k(benchmark, tmp_path)` - 10,000 variants
   d. `test_benchmark_excel_write_50k(benchmark, tmp_path)` - 50,000 variants (mark with @pytest.mark.slow)

   Each benchmark:
   - Creates DataFrame, writes TSV to tmp_path
   - Benchmarks `convert_to_excel(tsv_path, cfg, df=df)` followed by `finalize_excel_file(xlsx_path, cfg)`
   - Verifies output file exists and has correct row count

   e. `test_benchmark_excel_finalization_10k(benchmark, tmp_path)` - Benchmark ONLY finalize_excel_file on a pre-created 10K row Excel (measures hyperlink/formatting overhead)

   f. `test_benchmark_gt_preparsing_10k(benchmark)` - Benchmark parse_gt_column on 10K row DataFrame with GT column (measures the one-time parse cost)

3. **Ratio assertions (compare against openpyxl baseline):**
   - `test_xlsxwriter_vs_openpyxl_speedup(tmp_path)` - For 10K variants:
     - Time Excel write with xlsxwriter engine (current code via convert_to_excel)
     - Time Excel write with openpyxl engine (direct pd.to_excel with default engine)
     - Assert xlsxwriter is at least 1.5x faster (conservative; expect 2-5x)
     - Print actual speedup ratio for reporting

Mark all benchmarks with `@pytest.mark.slow` and `@pytest.mark.unit`. The ratio test does NOT use benchmark fixture (uses time.perf_counter for direct comparison).
  </action>
  <verify>
    `pytest tests/performance/benchmark_excel_output.py -v --no-header -x` passes (may skip slow tests without --runslow)
    `pytest tests/performance/benchmark_excel_output.py::test_xlsxwriter_vs_openpyxl_speedup -v` passes and prints speedup ratio
  </verify>
  <done>
    - 6+ benchmarks covering Excel write at 4 scales, finalization, and GT parsing
    - Ratio assertion proves xlsxwriter >= 1.5x faster than openpyxl
    - All benchmarks pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Create full Excel fidelity end-to-end test</name>
  <files>
    tests/unit/test_excel_full_fidelity.py
  </files>
  <action>
Create `tests/unit/test_excel_full_fidelity.py` - comprehensive test verifying full Excel output with all sheets and formatting.

1. `test_full_excel_with_all_sheets(tmp_path)`:
   - Create synthetic variants DataFrame (20 rows, 3 samples)
   - Include GT column, URL columns, CHROM, POS, REF, ALT, GENE, IMPACT
   - Write Results sheet via convert_to_excel()
   - Create metadata TSV, append via append_tsv_as_sheet(sheet_name="Metadata")
   - Create statistics TSV, append via append_tsv_as_sheet(sheet_name="Statistics")
   - Create gene burden TSV, append via append_tsv_as_sheet(sheet_name="Gene Burden")
   - Call finalize_excel_file() with config containing links={}
   - Open with openpyxl and verify:
     - All 4 sheets exist: Results, Metadata, Statistics, Gene Burden
     - Sheet order is correct (wb.sheetnames)
     - Results has correct number of data rows
     - Metadata/Statistics/Gene Burden have correct data

2. `test_full_excel_freeze_panes_all_sheets(tmp_path)`:
   - Same setup with all 4 sheets
   - Verify freeze_panes == "A2" on ALL sheets (not just Results)

3. `test_full_excel_auto_filter_all_sheets(tmp_path)`:
   - Verify auto_filter.ref is set on ALL sheets

4. `test_full_excel_url_hyperlinks(tmp_path)`:
   - Create DataFrame with 2 URL columns containing "https://..." values
   - After finalize, verify cells have hyperlink attribute set
   - Verify cell.style == "Hyperlink" for URL cells
   - Verify at least 70% of URL cells are converted (matching the 70% threshold in converter.py)

5. `test_gt_cache_not_in_excel(tmp_path)`:
   - Create DataFrame with _GT_PARSED column
   - Pass through convert_to_excel (which should receive clean df without cache cols)
   - Verify _GT_PARSED does not appear in any sheet header

Mark all with `@pytest.mark.unit`.
  </action>
  <verify>
    `pytest tests/unit/test_excel_full_fidelity.py -v` passes all tests
  </verify>
  <done>
    - 5 tests covering all-sheets generation, freeze panes, auto-filters, hyperlinks, cache cleanup
    - All tests pass proving functional equivalence of optimized Excel output
  </done>
</task>

</tasks>

<verification>
- `make ci-check` passes
- `pytest tests/performance/benchmark_excel_output.py -v` all benchmarks pass
- Ratio test confirms xlsxwriter >= 1.5x faster than openpyxl
- Full fidelity test proves all 4 sheets, formatting, and hyperlinks work correctly
</verification>

<success_criteria>
- Benchmarks exist at 100, 1K, 10K, 50K scales
- Ratio assertion proves measurable speedup (target 2-5x, assert >= 1.5x conservatively)
- Full fidelity test verifies: 4 sheets, freeze panes on all, auto-filters on all, hyperlinks working
- GT cache columns never appear in output
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/10-output-optimization/10-03-SUMMARY.md`
</output>
