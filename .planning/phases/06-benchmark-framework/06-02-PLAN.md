---
phase: 06-benchmark-framework
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - tests/performance/benchmark_inheritance.py
  - tests/performance/benchmark_comp_het.py
  - tests/performance/benchmark_genotype_replacement.py
  - tests/performance/benchmark_gene_burden.py
  - tests/performance/benchmark_scoring.py
  - tests/performance/benchmark_dataframe_io.py
autonomous: true

must_haves:
  truths:
    - "Inheritance analysis benchmark runs at 100, 1K, 10K variant scales with benchmark fixture"
    - "Compound het benchmark runs separately from general inheritance with dedicated test file"
    - "Genotype replacement benchmark tests both sequential and vectorized implementations"
    - "Gene burden benchmark exercises perform_gene_burden_analysis at multiple scales"
    - "Scoring benchmark exercises apply_scoring with synthetic config"
    - "DataFrame I/O benchmark measures TSV read/write performance at multiple sizes"
    - "All benchmarks store n_variants, n_samples, peak_memory_mb in extra_info metadata"
    - "All benchmarks run via `pytest tests/performance/ -k benchmark` without error"
  artifacts:
    - path: "tests/performance/benchmark_inheritance.py"
      provides: "Inheritance analysis micro/meso benchmarks"
      contains: "def test_"
      min_lines: 40
    - path: "tests/performance/benchmark_comp_het.py"
      provides: "Dedicated compound het benchmarks"
      contains: "compound_het"
      min_lines: 40
    - path: "tests/performance/benchmark_genotype_replacement.py"
      provides: "Genotype replacement benchmarks (sequential + vectorized)"
      contains: "replace_genotypes"
      min_lines: 40
    - path: "tests/performance/benchmark_gene_burden.py"
      provides: "Gene burden analysis benchmarks"
      contains: "gene_burden"
      min_lines: 40
    - path: "tests/performance/benchmark_scoring.py"
      provides: "Scoring benchmarks"
      contains: "apply_scoring"
      min_lines: 30
    - path: "tests/performance/benchmark_dataframe_io.py"
      provides: "DataFrame I/O benchmarks"
      contains: "read_csv"
      min_lines: 30
  key_links:
    - from: "tests/performance/benchmark_inheritance.py"
      to: "variantcentrifuge/inheritance/analyzer.py"
      via: "import"
      pattern: "from variantcentrifuge.inheritance"
    - from: "tests/performance/benchmark_comp_het.py"
      to: "variantcentrifuge/inheritance/comp_het_vectorized.py"
      via: "import"
      pattern: "from variantcentrifuge.inheritance.comp_het"
    - from: "tests/performance/benchmark_genotype_replacement.py"
      to: "variantcentrifuge/vectorized_replacer.py"
      via: "import"
      pattern: "from variantcentrifuge"
    - from: "tests/performance/benchmark_gene_burden.py"
      to: "variantcentrifuge/gene_burden.py"
      via: "import"
      pattern: "from variantcentrifuge.gene_burden"
    - from: "tests/performance/benchmark_scoring.py"
      to: "variantcentrifuge/scoring.py"
      via: "import"
      pattern: "from variantcentrifuge.scoring"
---

<objective>
Create component-level benchmark tests (micro/meso granularity) for all five target subsystems plus DataFrame I/O, using pytest-benchmark fixtures with parametrized variant counts and custom metadata.

Purpose: Establishes baseline performance measurements for each component before optimization begins. Covers BENCH-01 (benchmark suite), BENCH-03 (20% regression canary via --benchmark-compare-fail), and BENCH-06 (extra metadata).
Output: Six benchmark test files exercising inheritance, compound het, genotype replacement, gene burden, scoring, and DataFrame I/O.
</objective>

<execution_context>
@C:\Users\bernt\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\bernt\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-benchmark-framework/06-RESEARCH.md
@.planning/phases/06-benchmark-framework/06-CONTEXT.md
@.planning/phases/06-benchmark-framework/06-01-SUMMARY.md

# Source code being benchmarked:
@variantcentrifuge/inheritance/analyzer.py
@variantcentrifuge/inheritance/deducer.py
@variantcentrifuge/inheritance/comp_het.py
@variantcentrifuge/inheritance/comp_het_vectorized.py
@variantcentrifuge/gene_burden.py
@variantcentrifuge/scoring.py
@variantcentrifuge/replacer.py
@variantcentrifuge/vectorized_replacer.py

# Fixtures from Plan 01:
@tests/performance/conftest.py
@tests/performance/helpers/synthetic_data.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create inheritance, compound het, and genotype replacement benchmarks</name>
  <files>
    tests/performance/benchmark_inheritance.py
    tests/performance/benchmark_comp_het.py
    tests/performance/benchmark_genotype_replacement.py
  </files>
  <action>
    **benchmark_inheritance.py** — Micro/meso benchmarks for inheritance analysis:

    1. `test_deduce_patterns_scaling(benchmark, synthetic_variants, n_variants)`:
       - Parametrize n_variants: [100, 1000, 10000]
       - Use n_samples=3 (trio) for inheritance deduction
       - Generate synthetic pedigree (trio)
       - Build sample_list from pedigree keys
       - Benchmark `analyze_inheritance(df, pedigree_data, sample_list)` from variantcentrifuge.inheritance.analyzer
       - Store extra_info: n_variants, n_samples, component="inheritance_deduction"
       - Assert result is a DataFrame with Inheritance_Pattern column

    2. `test_deduce_single_variant_micro(benchmark, synthetic_variants)`:
       - Micro-level: benchmark `deduce_patterns_for_variant` on a single variant row (dict)
       - Use synthetic_variants(100, 3) to get a realistic row, convert first row to dict
       - Generate trio pedigree
       - Benchmark the single-variant function call
       - Store extra_info: component="deduce_single_variant"

    NOTE on inheritance: `analyze_inheritance` uses df.apply(axis=1) internally which calls deduce_patterns_for_variant per row. The micro benchmark tests the inner function directly; the meso benchmark tests the full orchestrator. Both are needed to identify where time goes.

    NOTE on function signatures: Study the actual function signatures in analyzer.py and deducer.py before writing benchmarks. The analyze_inheritance function takes (df, pedigree_data, sample_list, use_vectorized_comp_het). The deduce_patterns_for_variant function takes (variant_row, pedigree_data, sample_list). Make sure the synthetic data provides the correct column structure these functions expect — especially that GT column values contain per-sample genotypes that can be parsed.

    IMPORTANT: The GT column format for inheritance analysis is the POST-replacement format where each entry looks like "CHILD_001(0/1);FATHER_001(0/0);MOTHER_001(0/1)". The synthetic_variants fixture generates comma-separated raw GT format. You may need to transform the GT column to the post-replacement format for inheritance benchmarks, OR create a dedicated helper that generates inheritance-ready DataFrames.

    **benchmark_comp_het.py** — Dedicated compound het benchmarks:

    1. `test_comp_het_vectorized_scaling(benchmark, synthetic_variants, n_variants)`:
       - Parametrize n_variants: [100, 1000, 10000]
       - Use n_samples=3 (trio) for compound het detection
       - Filter synthetic data to a single gene (first gene in the GENE column) to simulate per-gene compound het analysis
       - Benchmark `analyze_gene_for_compound_het_vectorized(gene_df, pedigree_data, sample_list)`
       - Store extra_info: n_variants, n_genes=1, component="comp_het_vectorized"

    2. `test_comp_het_original_scaling(benchmark, synthetic_variants, n_variants)`:
       - Same as above but benchmark `analyze_gene_for_compound_het` (original, non-vectorized)
       - Parametrize n_variants: [100, 1000] — skip 10K for original (too slow)
       - Store extra_info: component="comp_het_original"

    NOTE: Compound het functions operate on a SINGLE GENE's DataFrame (all variants in one gene). The synthetic data needs to be filtered to one gene before calling. The gene_df must have appropriate columns including genotype data that the comp_het functions expect.

    **benchmark_genotype_replacement.py** — Genotype replacement benchmarks:

    1. `test_vectorized_replacement_scaling(benchmark, synthetic_variants, n_variants)`:
       - Parametrize n_variants: [100, 1000, 10000]
       - Use n_samples=100 (genotype replacement scales with sample count)
       - The vectorized replacer (`replace_genotypes_vectorized`) operates on a TSV file, not a DataFrame directly. Create a temporary TSV file from the synthetic DataFrame, then benchmark the vectorized replacer on it.
       - Study the actual function signature of `replace_genotypes_vectorized` in vectorized_replacer.py and build the appropriate cfg dict.
       - Store extra_info: n_variants, n_samples=100, component="genotype_replacement_vectorized"

    2. `test_sequential_replacement_scaling(benchmark, synthetic_variants, n_variants)`:
       - Parametrize n_variants: [100, 1000]
       - The sequential replacer (`replace_genotypes`) operates on an iterator of lines. Convert DataFrame to TSV lines iterator.
       - Study the actual function signature in replacer.py and build the appropriate cfg dict.
       - Store extra_info: component="genotype_replacement_sequential"

    IMPORTANT for genotype replacement: Both replacer functions need a `cfg` dict with specific keys (sample_list, extract_fields_separator, separator, genotype_replacement_map, append_extra_sample_fields, etc.). Study the actual source code to build a minimal but valid cfg. The sequential replacer yields lines (iterator), so you need to consume the iterator (e.g., `list(replace_genotypes(lines, cfg))`).

    For ALL benchmarks:
    - Mark with `@pytest.mark.performance`
    - Use `benchmark.extra_info['n_variants']` etc. for metadata (BENCH-06)
    - Use `benchmark.pedantic(..., rounds=3, iterations=1, warmup_rounds=1)` for expensive benchmarks (10K+ variants) to keep total runtime reasonable
    - For small sizes (100, 1000), default benchmark fixture auto-calibration is fine
    - Disable GC if tests are flaky: `@pytest.mark.benchmark(disable_gc=True)` — but try without first
  </action>
  <verify>
    - `pytest tests/performance/benchmark_inheritance.py --benchmark-disable -v` — all tests pass (--benchmark-disable skips timing, just runs the code)
    - `pytest tests/performance/benchmark_comp_het.py --benchmark-disable -v` — all tests pass
    - `pytest tests/performance/benchmark_genotype_replacement.py --benchmark-disable -v` — all tests pass
    - All three files import successfully: `python -c "import tests.performance.benchmark_inheritance"`
  </verify>
  <done>
    Three benchmark files covering inheritance (micro + meso), compound het (vectorized + original), and genotype replacement (vectorized + sequential) all execute successfully with pytest-benchmark and store custom metadata.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create gene burden, scoring, and DataFrame I/O benchmarks</name>
  <files>
    tests/performance/benchmark_gene_burden.py
    tests/performance/benchmark_scoring.py
    tests/performance/benchmark_dataframe_io.py
  </files>
  <action>
    **benchmark_gene_burden.py** — Gene burden analysis benchmarks:

    1. `test_gene_burden_analysis_scaling(benchmark, synthetic_gene_burden_data, n_variants)`:
       - Parametrize n_variants: [100, 1000, 10000]
       - Use synthetic_gene_burden_data fixture with n_samples=100, n_genes=50
       - Build a cfg dict matching gene_burden config format (see tests/conftest.py gene_burden_test_config fixture for the expected keys: gene_burden_mode, correction_method, confidence_interval_method, confidence_interval_alpha, continuity_correction)
       - Add required keys: case_samples, control_samples (from the fixture return value)
       - Benchmark `perform_gene_burden_analysis(df, cfg)` from variantcentrifuge.gene_burden
       - Store extra_info: n_variants, n_genes, n_samples, component="gene_burden"

    NOTE: Study `perform_gene_burden_analysis` signature carefully. It takes a DataFrame and cfg dict. The DataFrame must have the right column structure (GENE column at minimum, plus GT column with the specific format "SAMPLE_0001(0/1);SAMPLE_0002(0/0)" for per-sample parsing). The cfg must include case_samples and control_samples plus analysis parameters.

    **benchmark_scoring.py** — Scoring benchmarks:

    1. `test_scoring_apply_scaling(benchmark, synthetic_variants, synthetic_scoring_config, n_variants)`:
       - Parametrize n_variants: [100, 1000, 10000]
       - Use synthetic_variants with n_samples=10 (scoring is variant-count-sensitive, not sample-count)
       - Benchmark `apply_scoring(df, scoring_config)` from variantcentrifuge.scoring
       - Store extra_info: n_variants, n_formulas (from config), component="scoring"

    NOTE: The scoring function uses pd.eval to apply formulas. The synthetic_scoring_config must define variable mappings that map to actual DataFrame column names. The variables dict maps original_column -> {var_name, default_value}. Make sure the synthetic DataFrame has columns that match the variable mappings in the synthetic config.

    **benchmark_dataframe_io.py** — DataFrame read/write benchmarks:

    1. `test_csv_read_scaling(benchmark, synthetic_variants, tmp_path, n_variants)`:
       - Parametrize n_variants: [1000, 10000, 50000]
       - Generate DataFrame, write to TSV file in tmp_path (setup, outside benchmark)
       - Benchmark `pd.read_csv(path, sep='\t')` — this is the hot path for all pipeline data loading
       - Store extra_info: n_variants, n_columns, file_size_mb, component="dataframe_read"

    2. `test_csv_write_scaling(benchmark, synthetic_variants, tmp_path, n_variants)`:
       - Parametrize n_variants: [1000, 10000, 50000]
       - Generate DataFrame in setup
       - Benchmark `df.to_csv(path, sep='\t', index=False)` — output generation hot path
       - Store extra_info: n_variants, n_columns, component="dataframe_write"

    3. `test_pyarrow_read_scaling(benchmark, synthetic_variants, tmp_path, n_variants)`:
       - Same as test_csv_read but with `pd.read_csv(path, sep='\t', engine='pyarrow')`
       - This establishes the baseline for Phase 8's PyArrow optimization
       - If pyarrow is not installed, skip with `pytest.importorskip("pyarrow")`
       - Store extra_info: engine="pyarrow", component="dataframe_read_pyarrow"

    For ALL benchmarks:
    - Mark with `@pytest.mark.performance`
    - Use `benchmark.extra_info` for all metadata (BENCH-06)
    - For 50K variant tests, use pedantic mode to control iterations
    - For I/O tests, use pedantic mode with setup function to ensure file is created fresh: `benchmark.pedantic(fn, setup=setup_fn, rounds=3, iterations=1, warmup_rounds=1)`
  </action>
  <verify>
    - `pytest tests/performance/benchmark_gene_burden.py --benchmark-disable -v` — all tests pass
    - `pytest tests/performance/benchmark_scoring.py --benchmark-disable -v` — all tests pass
    - `pytest tests/performance/benchmark_dataframe_io.py --benchmark-disable -v` — all tests pass
    - All three files import successfully
  </verify>
  <done>
    Gene burden, scoring, and DataFrame I/O benchmark tests execute at parametrized scales with custom metadata. Coverage spans all five required components (BENCH-01) plus DataFrame I/O baseline for future PyArrow optimization comparison.
  </done>
</task>

</tasks>

<verification>
- `pytest tests/performance/ --benchmark-disable -v` — ALL benchmark tests pass (code correctness, ignoring timing)
- `pytest tests/performance/ -v --benchmark-only -k "100 or 1000"` — small-scale benchmarks complete in < 5 minutes with timing data
- Each benchmark file stores extra_info metadata (grep for "extra_info" in all benchmark files)
- No private identifiers in any test file
</verification>

<success_criteria>
1. All six benchmark files exist and pass with --benchmark-disable
2. Benchmarks cover inheritance, comp_het, genotype replacement, gene burden, scoring, and DataFrame I/O (BENCH-01)
3. All benchmarks store n_variants, n_samples, component in extra_info (BENCH-06)
4. Parametrized tests cover 100, 1K, 10K variant scales (50K for I/O only)
5. Both vectorized and sequential implementations are benchmarked where applicable
</success_criteria>

<output>
After completion, create `.planning/phases/06-benchmark-framework/06-02-SUMMARY.md`
</output>
