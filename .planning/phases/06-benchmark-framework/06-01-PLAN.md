---
phase: 06-benchmark-framework
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - benchmarks/.gitignore
  - .gitignore
  - tests/performance/__init__.py
  - tests/performance/conftest.py
  - tests/performance/helpers/__init__.py
  - tests/performance/helpers/synthetic_data.py
  - tests/performance/helpers/memory_budgets.py
autonomous: true

must_haves:
  truths:
    - "pytest-benchmark is installable via dev dependencies"
    - "Synthetic data generators produce identical DataFrames given the same seed"
    - "Generated DataFrames match variantcentrifuge column structure (CHROM, POS, REF, ALT, GT, GENE, FILTER, EFFECT, IMPACT)"
    - "Pedigree fixtures produce trio structures with configurable extra samples"
    - "No private cohort names or traceable identifiers appear anywhere in committed code"
    - "benchmarks/ directory is gitignored"
  artifacts:
    - path: "pyproject.toml"
      provides: "pytest-benchmark dev dependency"
      contains: "pytest-benchmark"
    - path: "benchmarks/.gitignore"
      provides: "Gitignore for ephemeral benchmark results"
      contains: "*"
    - path: "tests/performance/conftest.py"
      provides: "Benchmark fixtures: synthetic_variants, synthetic_pedigree, synthetic_scoring_config"
      exports: ["synthetic_variants", "synthetic_pedigree", "synthetic_scoring_config"]
    - path: "tests/performance/helpers/synthetic_data.py"
      provides: "Synthetic data generation functions"
      min_lines: 100
    - path: "tests/performance/helpers/memory_budgets.py"
      provides: "tracemalloc context manager and budget warning helper"
      min_lines: 30
  key_links:
    - from: "tests/performance/conftest.py"
      to: "tests/performance/helpers/synthetic_data.py"
      via: "import"
      pattern: "from .helpers.synthetic_data import"
    - from: "tests/performance/conftest.py"
      to: "tests/performance/helpers/memory_budgets.py"
      via: "import"
      pattern: "from .helpers.memory_budgets import"
---

<objective>
Create the benchmark framework foundation: add pytest-benchmark dependency, build synthetic data generators for genomic DataFrames and pedigrees, create tracemalloc memory budget helpers, and set up the project structure for benchmark result storage.

Purpose: All subsequent benchmark plans depend on these fixtures and helpers. Without reproducible synthetic data, benchmarks are meaningless. Without the dependency installed, nothing runs.
Output: Installable dev deps, conftest.py with factory fixtures, helpers/ module with synthetic data + memory profiling utilities, gitignored benchmarks/ directory.
</objective>

<execution_context>
@C:\Users\bernt\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\bernt\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-benchmark-framework/06-RESEARCH.md
@.planning/phases/06-benchmark-framework/06-CONTEXT.md

# Existing code for reference:
@tests/conftest.py
@pyproject.toml
@variantcentrifuge/inheritance/analyzer.py
@variantcentrifuge/inheritance/deducer.py
@variantcentrifuge/gene_burden.py
@variantcentrifuge/scoring.py
@variantcentrifuge/replacer.py
@variantcentrifuge/vectorized_replacer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add pytest-benchmark dependency and create project structure</name>
  <files>
    pyproject.toml
    benchmarks/.gitignore
    .gitignore
    tests/performance/__init__.py
    tests/performance/helpers/__init__.py
  </files>
  <action>
    1. Add `pytest-benchmark>=5.1.0` to `[project.optional-dependencies] dev` in pyproject.toml. Do NOT add the [histogram] extra (matplotlib is heavy and not needed for core benchmarking).

    2. Create `benchmarks/.gitignore` containing just `*` and `!.gitignore` — this gitignores all JSON result files while keeping the directory in git.

    3. Append `benchmarks/*.json` to the root `.gitignore` as a belt-and-suspenders approach (some git clients ignore nested .gitignore).

    4. Create `tests/performance/__init__.py` as an empty file (the directory already exists but has no __init__).

    5. Create `tests/performance/helpers/__init__.py` as an empty file.

    6. Run `uv pip install -e ".[dev]"` to install the new dependency.

    7. Verify installation: `python -c "import pytest_benchmark; print(pytest_benchmark.__version__)"` must succeed.

    IMPORTANT: The existing files in tests/performance/ (benchmark_pipeline.py, test_streaming_parallel_performance.py, README.md) are pre-GSD legacy code. Do NOT modify or remove them — they still serve as standalone scripts. The new pytest-benchmark-based framework coexists alongside them.
  </action>
  <verify>
    - `python -c "import pytest_benchmark"` exits 0
    - `ls benchmarks/.gitignore` exists
    - `ls tests/performance/__init__.py` exists
    - `ls tests/performance/helpers/__init__.py` exists
    - `grep pytest-benchmark pyproject.toml` returns a match
  </verify>
  <done>pytest-benchmark is installed, benchmarks/ directory is gitignored, tests/performance/ is a proper Python package with helpers/ subpackage</done>
</task>

<task type="auto">
  <name>Task 2: Create synthetic data generators and conftest fixtures</name>
  <files>
    tests/performance/helpers/synthetic_data.py
    tests/performance/helpers/memory_budgets.py
    tests/performance/conftest.py
  </files>
  <action>
    **synthetic_data.py** — Create module with these functions:

    1. `generate_synthetic_variants(n_variants, n_samples, seed=42) -> pd.DataFrame`:
       - Use `numpy.random.default_rng(seed)` for reproducibility
       - Generate columns matching variantcentrifuge DataFrame structure:
         - CHROM: weighted distribution favoring chr1-10 (use string "1"-"22", "X", "Y")
         - POS: integers 1 to 250,000,000
         - REF/ALT: single nucleotide bases (A/C/G/T), ensure REF != ALT per row
         - GT: comma-separated genotype string per variant, format "sample1_gt,sample2_gt,...". Genotypes are "0/0", "0/1", "1/1" with realistic allele frequency distribution (90% rare variants with 95/4/1% 0-0/0-1/1-1 split, 8% uncommon with 80/15/5, 2% common with 50/40/10)
         - GENE: synthetic gene names like "GENE_0001" through "GENE_NNNN" — use ~n_variants/10 unique genes so there are ~10 variants per gene on average
         - FILTER: 95% "PASS", 5% "LowQual"
         - EFFECT: random from ["missense_variant", "synonymous_variant", "frameshift_variant", "stop_gained", "splice_region_variant"] with realistic weights [0.40, 0.30, 0.10, 0.05, 0.15]
         - IMPACT: correlated with EFFECT — HIGH for frameshift/stop_gained, MODERATE for missense, LOW for synonymous, MODIFIER for splice_region (map deterministically from EFFECT)
       - Sort by CHROM (natural sort: 1,2,...22,X,Y), then POS
       - Return DataFrame with reset index
       - CRITICAL: No private identifiers. All sample names must be "SAMPLE_NNNN" format.

    2. `generate_synthetic_pedigree(n_samples=3, seed=42) -> dict`:
       - Minimum trio: CHILD_001 (affected), FATHER_001 (unaffected), MOTHER_001 (unaffected)
       - Additional samples added as unrelated (father_id=0, mother_id=0)
       - Sample IDs are "SAMPLE_NNNN" for extras (matching generate_synthetic_variants sample names)
       - Return dict[str, dict] matching variantcentrifuge pedigree_data format:
         `{sample_id: {sample_id, father_id, mother_id, sex, affected_status}}`

    3. `generate_synthetic_scoring_config() -> dict`:
       - Return a minimal scoring config matching variantcentrifuge scoring format:
         - variables: map IMPACT -> "impact_var", EFFECT -> "effect_var" with numeric defaults
         - formulas: 1-2 simple formulas using pd.eval compatible syntax
       - This enables scoring benchmarks without reading real scoring config files from disk.

    4. `generate_synthetic_gene_burden_data(n_variants, n_samples, n_genes=50, seed=42) -> tuple[pd.DataFrame, set, set]`:
       - Generate a DataFrame with GT column in the format gene_burden expects: "SAMPLE_0001(0/1);SAMPLE_0002(0/0);..."
       - Split samples into case_samples and control_samples sets (50/50 split)
       - Return (df, case_samples, control_samples)

    **memory_budgets.py** — Create module with:

    1. `class MemoryTracker` context manager:
       - On enter: starts tracemalloc, resets peak
       - On exit: captures peak memory, stops tracemalloc
       - Properties: `peak_mb`, `current_mb`

    2. `def warn_if_over_budget(peak_mb: float, budget_mb: float, context: str)`:
       - Issues `warnings.warn()` if peak_mb > budget_mb
       - Includes context string in warning message
       - NEVER raises an exception — memory violations are warnings only per CONTEXT.md

    3. Default budget constants:
       - `INHERITANCE_BUDGET_MB = 512`
       - `COMP_HET_BUDGET_MB = 256`
       - `GENOTYPE_REPLACEMENT_BUDGET_MB = 1024`
       - `GENE_BURDEN_BUDGET_MB = 512`
       - `SCORING_BUDGET_MB = 256`
       These are initial estimates at 2x expected peak for 10K variants x 100 samples. They will be refined once actual profiling data exists.

    **conftest.py** — Create pytest conftest with factory fixtures:

    1. `@pytest.fixture` `synthetic_variants` — factory fixture wrapping `generate_synthetic_variants`. Returns a callable `_generate(n_variants, n_samples, seed=42)`.

    2. `@pytest.fixture` `synthetic_pedigree` — factory fixture wrapping `generate_synthetic_pedigree`. Returns a callable `_generate(n_samples=3, seed=42)`.

    3. `@pytest.fixture` `synthetic_scoring_config` — calls `generate_synthetic_scoring_config()` directly (not a factory — config is always the same).

    4. `@pytest.fixture` `synthetic_gene_burden_data` — factory fixture wrapping `generate_synthetic_gene_burden_data`. Returns a callable `_generate(n_variants, n_samples, n_genes=50, seed=42)`.

    5. `@pytest.fixture` `memory_tracker` — returns a fresh MemoryTracker instance.

    IMPORTANT: Do NOT modify the existing tests/conftest.py (root conftest). The performance conftest is LOCAL to tests/performance/ and provides benchmark-specific fixtures only. The root conftest already auto-marks performance tests with `pytest.mark.slow` and `pytest.mark.performance`.
  </action>
  <verify>
    - `python -c "from tests.performance.helpers.synthetic_data import generate_synthetic_variants; df = generate_synthetic_variants(100, 10); print(df.shape, list(df.columns))"` — prints (100, 9) with expected columns
    - `python -c "from tests.performance.helpers.synthetic_data import generate_synthetic_variants; df1 = generate_synthetic_variants(100, 10, seed=42); df2 = generate_synthetic_variants(100, 10, seed=42); assert df1.equals(df2); print('Reproducible: OK')"` — confirms seed reproducibility
    - `python -c "from tests.performance.helpers.synthetic_data import generate_synthetic_pedigree; p = generate_synthetic_pedigree(10); print(len(p), list(p.keys())[:5])"` — prints 10 with CHILD_001, FATHER_001, MOTHER_001 and SAMPLE_ prefixed extras
    - `python -c "from tests.performance.helpers.memory_budgets import MemoryTracker; m = MemoryTracker(); print('MemoryTracker: OK')"` — imports successfully
    - `pytest tests/performance/ --collect-only 2>&1 | head -20` — collects test items (may be 0 tests initially, but conftest loads without error)
    - Grep all files in tests/performance/ for any real patient names, cohort references, or non-synthetic identifiers — must find zero matches
  </verify>
  <done>
    Synthetic data generators produce reproducible DataFrames at configurable sizes with realistic genomic distributions. All identifiers are fully synthetic. conftest.py provides factory fixtures for all benchmark tests. Memory tracking helper wraps tracemalloc with warning-only budget enforcement.
  </done>
</task>

</tasks>

<verification>
- `uv pip install -e ".[dev]"` completes without error
- `python -c "import pytest_benchmark"` succeeds
- `pytest tests/performance/ --collect-only` loads conftest without errors
- Synthetic data is reproducible: same seed -> identical DataFrame
- No private identifiers in any committed file
- benchmarks/ directory exists and is gitignored
</verification>

<success_criteria>
1. pytest-benchmark is installed and importable
2. Synthetic data fixtures generate DataFrames matching variantcentrifuge column structure at 100/1K/10K/50K variants
3. Pedigree fixtures produce trio + cohort structures
4. Memory tracker provides tracemalloc-based peak tracking with warning-only budget enforcement
5. All identifiers are fully synthetic (SAMPLE_NNNN, GENE_NNNN, CHILD_001, etc.)
6. benchmarks/ directory is gitignored for ephemeral JSON results
</success_criteria>

<output>
After completion, create `.planning/phases/06-benchmark-framework/06-01-SUMMARY.md`
</output>
