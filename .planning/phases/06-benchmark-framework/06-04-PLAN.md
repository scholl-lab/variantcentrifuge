---
phase: 06-benchmark-framework
plan: 04
type: execute
wave: 3
depends_on: ["06-02", "06-03"]
files_modified:
  - tests/performance/helpers/result_diff.py
  - tests/performance/benchmark_pipeline_macro.py
autonomous: true

must_haves:
  truths:
    - "Result diff helper loads two pytest-benchmark JSON files and prints a comparison table"
    - "Diff output shows benchmark name, baseline time, current time, and percentage change"
    - "Macro pipeline benchmarks exercise the inheritance analysis orchestrator at cohort scale"
    - "All benchmark tests pass together via `pytest tests/performance/ --benchmark-disable`"
    - "Benchmark suite runs via `pytest -m performance` marker"
    - "JSON results can be saved with --benchmark-save and compared with the diff helper"
  artifacts:
    - path: "tests/performance/helpers/result_diff.py"
      provides: "JSON benchmark result comparison utility"
      min_lines: 40
    - path: "tests/performance/benchmark_pipeline_macro.py"
      provides: "Macro-level pipeline benchmarks"
      contains: "def test_"
      min_lines: 40
  key_links:
    - from: "tests/performance/helpers/result_diff.py"
      to: "benchmarks/"
      via: "file read"
      pattern: "json.load"
    - from: "tests/performance/benchmark_pipeline_macro.py"
      to: "variantcentrifuge/inheritance/analyzer.py"
      via: "import"
      pattern: "from variantcentrifuge"
---

<objective>
Create the result diff comparison helper and macro-level pipeline benchmarks, then verify the complete benchmark suite works end-to-end with save/compare workflow.

Purpose: The result diff helper (CONTEXT.md requirement) enables developers to quickly see what got faster/slower between optimization runs. Macro benchmarks test the full analysis pipeline at cohort scale. This plan also verifies the entire benchmark framework works as a cohesive suite.
Output: Result diff utility, macro pipeline benchmark, verified end-to-end benchmark workflow.
</objective>

<execution_context>
@C:\Users\bernt\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\bernt\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/06-benchmark-framework/06-RESEARCH.md
@.planning/phases/06-benchmark-framework/06-CONTEXT.md
@.planning/phases/06-benchmark-framework/06-01-SUMMARY.md
@.planning/phases/06-benchmark-framework/06-02-SUMMARY.md
@.planning/phases/06-benchmark-framework/06-03-SUMMARY.md

# Fixtures:
@tests/performance/conftest.py
@tests/performance/helpers/synthetic_data.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create result diff helper and macro pipeline benchmarks</name>
  <files>
    tests/performance/helpers/result_diff.py
    tests/performance/benchmark_pipeline_macro.py
  </files>
  <action>
    **result_diff.py** — Simple comparison utility for benchmark results:

    1. `def compare_benchmark_results(baseline_path: str, current_path: str) -> None`:
       - Load two pytest-benchmark JSON result files
       - pytest-benchmark saves results in `.benchmarks/` (inside the project, configurable) with structure: `{"benchmarks": [{"name": ..., "stats": {"mean": ..., "stddev": ...}, "extra_info": {...}}, ...]}`
       - Extract benchmark name and mean time from each
       - Match benchmarks by name between baseline and current
       - Print a formatted table showing:
         - Benchmark name (truncated to 50 chars if needed)
         - Baseline mean time
         - Current mean time
         - Percentage change (positive = slower, negative = faster)
         - Status: FASTER (green), SLOWER (red), SAME (within 5%)
       - Use ANSI color codes for terminal output (green for faster, red for slower, reset code)
       - Show NEW benchmarks (in current but not baseline) and REMOVED (in baseline but not current)
       - Print summary line: "X faster, Y slower, Z unchanged, N new, M removed"

    2. `def load_benchmark_json(path: str) -> dict`:
       - Load and validate a pytest-benchmark JSON file
       - Raise FileNotFoundError with helpful message if file doesn't exist
       - Raise ValueError if JSON structure doesn't have 'benchmarks' key

    3. Add `if __name__ == "__main__":` block:
       - Accept two positional arguments: baseline_file, current_file
       - Call compare_benchmark_results
       - Usage: `python -m tests.performance.helpers.result_diff baseline.json current.json`

    Keep it simple — this is sprint tooling per CONTEXT.md. No histograms, no fancy formatting, just a clear table.

    **benchmark_pipeline_macro.py** — Macro-level benchmarks:

    Macro benchmarks test the full analysis orchestration at cohort scale. They do NOT invoke external tools (bcftools, SnpSift) — Python code paths only per CONTEXT.md. The "macro" here means the full inheritance analysis pipeline (which is the dominant cost: 40-60% of total time).

    1. `test_full_inheritance_analysis_cohort(benchmark, synthetic_variants)`:
       - Generate data: n_variants=5000, n_samples=100 (medium cohort)
       - Generate pedigree with 100 samples (trio + 97 unrelated)
       - Transform GT column to post-replacement format ("SAMPLE_0001(0/1);SAMPLE_0002(0/0);...")
       - Benchmark `analyze_inheritance(df, pedigree_data, sample_list)` with pedantic mode (rounds=3, iterations=1, warmup_rounds=1)
       - Store extra_info: n_variants=5000, n_samples=100, n_genes (computed from unique GENE values), level="macro", component="full_inheritance"
       - Assert result DataFrame has Inheritance_Pattern column

    2. `test_full_inheritance_analysis_large_cohort(benchmark, synthetic_variants)`:
       - Generate data: n_variants=10000, n_samples=500 (large cohort — variantcentrifuge's primary use case)
       - Generate pedigree with 500 samples
       - Same structure as above with pedantic mode
       - Store extra_info: level="macro", component="full_inheritance_large"
       - Mark with `@pytest.mark.slow` in addition to `@pytest.mark.performance` — this test may take > 60 seconds

    3. `test_gene_burden_full_pipeline(benchmark, synthetic_gene_burden_data)`:
       - Generate data: n_variants=5000, n_samples=200, n_genes=100
       - Build full cfg dict for gene burden analysis
       - Benchmark `perform_gene_burden_analysis(df, cfg)` with pedantic mode
       - Store extra_info: level="macro", component="full_gene_burden"

    IMPORTANT: The macro benchmarks may be slow (30-120 seconds each). Use pedantic mode with low round counts. Mark the large cohort test as `@pytest.mark.slow` so it can be skipped with `pytest -m "performance and not slow"` during quick iterations.

    For ALL tests: Mark with `@pytest.mark.performance`.
  </action>
  <verify>
    - `python -c "from tests.performance.helpers.result_diff import compare_benchmark_results; print('Import OK')"` — imports successfully
    - `pytest tests/performance/benchmark_pipeline_macro.py --benchmark-disable -v -k "not large_cohort"` — non-slow macro tests pass
    - `pytest tests/performance/benchmark_pipeline_macro.py --benchmark-disable -v -k "large_cohort"` — large cohort test passes (may take 30+ seconds)
  </verify>
  <done>
    Result diff helper loads and compares pytest-benchmark JSON files with color-coded table output. Macro benchmarks exercise full inheritance analysis and gene burden at cohort scale (100-500 samples). All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: End-to-end benchmark suite verification and save/compare workflow test</name>
  <files>
    (no new files — verification only)
  </files>
  <action>
    Verify the complete benchmark framework works as a cohesive suite:

    1. Run the full benchmark suite (small scale only for speed):
       ```
       pytest tests/performance/ -v -k "100 or 1000 or micro or ratio or memory" --benchmark-save=verification_baseline
       ```
       This should:
       - Collect all benchmark tests from all 8 benchmark files
       - Execute benchmarks with timing (not --benchmark-disable)
       - Save results to .benchmarks/ directory as JSON
       - All tests should PASS

    2. Run the suite again to test comparison:
       ```
       pytest tests/performance/ -v -k "100 or 1000 or micro or ratio or memory" --benchmark-save=verification_current --benchmark-compare=verification_baseline --benchmark-compare-fail=mean:20%
       ```
       This should:
       - Compare against the baseline saved in step 1
       - Detect if any benchmark regressed by 20%+ (should not happen since code is identical)
       - Produce comparison output in console

    3. Test the result diff helper on the saved files:
       - Find the saved JSON files in `.benchmarks/` directory
       - Run: `python -m tests.performance.helpers.result_diff <baseline_json> <current_json>`
       - Verify it prints a comparison table showing ~0% change for all benchmarks

    4. Verify marker-based running:
       - `pytest -m performance --collect-only` — should list all benchmark tests
       - `pytest -m "performance and not slow" --collect-only` — should exclude large cohort tests
       - `pytest tests/performance/ --collect-only` — should list all tests including legacy ones

    5. Clean up: Remove .benchmarks/ verification files (they were just for testing the workflow):
       ```
       rm -rf .benchmarks/
       ```

    6. Final verification summary — confirm all requirements are met:
       - BENCH-01: Benchmark suite covers inheritance, genotype replacement, gene burden, scoring, DataFrame I/O (check file count and coverage)
       - BENCH-02: Synthetic data generators produce reproducible DataFrames (confirmed in Plan 01)
       - BENCH-03: --benchmark-compare-fail=mean:20% detects regressions (confirmed in step 2)
       - BENCH-04: Ratio assertions compare vectorized vs sequential in same run (confirmed in Plan 03)
       - BENCH-05: Memory budget assertions via tracemalloc with warning-only (confirmed in Plan 03)
       - BENCH-06: All benchmarks include extra_info metadata (grep for extra_info across all files)

    If any step fails, diagnose and fix the issue. Common problems:
    - Import errors: missing __init__.py or wrong import path
    - Fixture not found: conftest.py not in right directory
    - Benchmark fixture conflicts with tracemalloc: these should be in separate files
    - pytest-benchmark save path issues on Windows: use forward slashes
  </action>
  <verify>
    - `pytest tests/performance/ --benchmark-disable -v` — ALL tests pass (correctness check)
    - `pytest tests/performance/ -k "100" --benchmark-save=test_run -v` — saves results, no errors
    - `pytest -m performance --collect-only | tail -5` — shows total collected tests count (should be 25+)
    - All 6 BENCH requirements confirmed met
  </verify>
  <done>
    Complete benchmark framework verified end-to-end: all tests pass, save/compare workflow works, result diff helper produces comparison output, all BENCH-01 through BENCH-06 requirements confirmed met. Framework is ready for Phase 7+ optimization work.
  </done>
</task>

</tasks>

<verification>
- Full suite: `pytest tests/performance/ --benchmark-disable -v` passes all tests
- Marker filter: `pytest -m performance --collect-only` lists all benchmark tests
- Save/compare: `--benchmark-save` and `--benchmark-compare-fail=mean:20%` work together
- Result diff: `python -m tests.performance.helpers.result_diff` produces formatted output
- Coverage: All 5 components + DataFrame I/O benchmarked at micro/meso/macro levels
- All BENCH requirements (01-06) confirmed met
</verification>

<success_criteria>
1. Result diff helper compares two JSON files and prints color-coded table (CONTEXT.md requirement)
2. Macro benchmarks exercise full inheritance and gene burden at 100-500 sample cohort scale
3. Complete benchmark suite passes end-to-end with save/compare workflow
4. All BENCH-01 through BENCH-06 requirements verified met
5. Benchmark framework is ready for Phase 7+ optimization baseline measurements
</success_criteria>

<output>
After completion, create `.planning/phases/06-benchmark-framework/06-04-SUMMARY.md`
</output>
