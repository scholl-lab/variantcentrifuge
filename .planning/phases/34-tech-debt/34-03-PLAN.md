---
phase: 34-tech-debt
plan: 03
type: execute
wave: 2
depends_on: ["34-02"]
files_modified:
  - tests/unit/test_coast_python_comparison.py
  - scripts/generate_coast_golden.R
autonomous: false

must_haves:
  truths:
    - "COAST Python output for GCKD-derived gene scenarios matches R AllelicSeries output within 10% relative difference"
    - "Golden values are hardcoded in the test file (not computed at test time)"
    - "Test covers at least 3 scenarios: all categories present, partial categories, single-variant gene"
  artifacts:
    - path: "tests/unit/test_coast_python_comparison.py"
      provides: "GCKD golden comparison test class"
      contains: "TestCOASTGCKDGolden"
    - path: "scripts/generate_coast_golden.R"
      provides: "R script to generate golden values from GCKD data"
      contains: "AllelicSeries"
  key_links:
    - from: "tests/unit/test_coast_python_comparison.py"
      to: "variantcentrifuge/association/tests/allelic_series_python.py"
      via: "PurePythonCOASTTest.run() called with GCKD-derived inputs"
      pattern: "PurePythonCOASTTest"
---

<objective>
Generate COAST R golden values from GCKD testing data and create a Python test that asserts 10% relative tolerance (TD-03).

Purpose: One-time validation that Python COAST implementation matches R AllelicSeries reference. Per CONTEXT, this is a validation exercise, not permanent CI infrastructure — but the test class stays in the codebase as a regression guard.
Output: R script for golden value generation, hardcoded golden values in test file.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/34-tech-debt/34-RESEARCH.md
@tests/unit/test_coast_python_comparison.py
@scripts/generate_coast_golden.R
@variantcentrifuge/association/tests/allelic_series_python.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create R script for GCKD golden value generation and test scaffold</name>
  <files>scripts/generate_coast_golden.R, tests/unit/test_coast_python_comparison.py</files>
  <action>
**R script (scripts/generate_coast_golden.R):**

Extend or create a section in the existing generate_coast_golden.R script that:
1. Reads GCKD variant data (the script should document where data comes from — `testing/` directory)
2. For each test gene scenario, runs R AllelicSeries::COAST() and captures the omnibus p-value
3. Covers at minimum these scenario types:
   - All 3 categories present (BMV, DMV, PTV all have variants)
   - 1-2 categories missing (partial category)
   - Single-variant gene
   - All variants in one category
   - Tied SIFT/PolyPhen scores across categories
4. Prints the golden values in a format easy to copy into Python (e.g., dict literal)

**Test scaffold (tests/unit/test_coast_python_comparison.py):**

Add a new test class `TestCOASTGCKDGoldenComparison` below the existing tests. Structure:
1. Define `_GCKD_GOLDEN_SCENARIOS` as a dict mapping scenario_name -> dict with keys:
   - `genotypes`: numpy array (samples x variants)
   - `phenotype`: numpy array (binary)
   - `categories`: list of ints (1=BMV, 2=DMV, 3=PTV per variant)
   - `r_omnibus_p`: float (the R golden value — placeholder initially)
   - `description`: str
2. Each test method runs PurePythonCOASTTest on the scenario inputs and asserts:
   `abs(python_p - r_p) / r_p < 0.10` (10% relative tolerance)
3. Use `@pytest.mark.parametrize` over the scenarios dict
4. Mark with `@pytest.mark.unit`

**IMPORTANT:** Use placeholder R values (e.g., `r_omnibus_p=0.999`) initially. The actual values will be filled in after the user runs the R script (Task 2 checkpoint). Structure the test inputs from GCKD data characteristics documented in testing/ directory.

The scenario inputs (genotypes, phenotypes, categories) should be realistic small extracts that represent the GCKD data patterns — not random synthetic data. Use the existing test infrastructure patterns from the 5 existing `_R_GOLDEN_SCENARIO_*` constants as a template.

Use the new column naming convention from Plan 02: `coast_pvalue` (not `coast_p_value`).
  </action>
  <verify>Run `pytest tests/unit/test_coast_python_comparison.py::TestCOASTGCKDGoldenComparison -v --no-header` — tests should run (they will fail on tolerance with placeholder values, which is expected).</verify>
  <done>R script exists for generating GCKD golden values. Test class scaffold exists with realistic scenario inputs and placeholder R values.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>R script for GCKD golden value generation and Python test scaffold with placeholder values.</what-built>
  <how-to-verify>
1. Review `scripts/generate_coast_golden.R` — does it cover the right scenarios?
2. Run the R script against GCKD testing data: `Rscript scripts/generate_coast_golden.R`
3. Copy the R golden p-values into the test file, replacing placeholder values
4. Run `pytest tests/unit/test_coast_python_comparison.py::TestCOASTGCKDGoldenComparison -v`
5. If any scenario fails the 10% tolerance, investigate the discrepancy
  </how-to-verify>
  <resume-signal>Type "golden values filled" with the R output, or describe any discrepancies found.</resume-signal>
</task>

</tasks>

<verification>
- R script runs without error on GCKD data
- TestCOASTGCKDGoldenComparison passes with real R golden values filled in
- `make test-fast` passes (no regressions)
</verification>

<success_criteria>
- COAST Python output matches R AllelicSeries within 10% relative tolerance for all GCKD scenarios
- Golden values are hardcoded in test file (not computed at runtime)
- At least 3 distinct scenario types covered (all categories, partial, single-variant)
</success_criteria>

<output>
After completion, create `.planning/phases/34-tech-debt/34-03-SUMMARY.md`
</output>
