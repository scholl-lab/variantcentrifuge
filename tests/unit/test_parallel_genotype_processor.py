"""
Unit tests for the enhanced parallel genotype processor.

Tests cover the new streaming pipeline architecture, adaptive memory management,
and intelligent chunking strategies.
"""

import tempfile
from pathlib import Path
from unittest.mock import Mock, patch

import numpy as np
import pandas as pd
import pytest

from variantcentrifuge.parallel_genotype_processor import (
    AdaptiveMemoryManager,
    IntelligentChunkCalculator,
    StreamingGenotypeProcessor,
    process_streaming_parallel,
)


class TestAdaptiveMemoryManager:
    """Test adaptive memory management and profiling."""

    def test_memory_manager_initialization(self):
        """Test memory manager initializes correctly."""
        manager = AdaptiveMemoryManager()
        assert manager.memory_samples == []
        assert manager.processing_stats == {}
        assert manager.baseline_memory is None

    @patch("psutil.Process")
    def test_profile_chunk_memory(self, mock_process):
        """Test memory profiling of chunk processing."""
        # Mock memory info progression
        mock_proc = Mock()
        mock_proc.memory_info.return_value.rss = 1000000000  # 1GB base
        mock_process.return_value = mock_proc

        manager = AdaptiveMemoryManager()

        # Create test chunk
        test_data = pd.DataFrame(
            {"GT": ["0/1,1/1,0/0", "1/1,0/1,1/0"] * 50, "other_col": ["data"] * 100}
        )

        stats = manager.profile_chunk_memory(test_data, sample_count=3)

        # Verify stats structure
        assert "bytes_per_row_actual" in stats
        assert "pandas_overhead_actual" in stats
        assert "sample_efficiency" in stats
        assert len(manager.memory_samples) == 1

    def test_get_realistic_memory_estimate_no_samples(self):
        """Test memory estimation without profiling history."""
        manager = AdaptiveMemoryManager()
        estimates = manager.get_realistic_memory_estimate(sample_count=100)

        assert "bytes_per_row" in estimates
        assert "pandas_overhead" in estimates
        assert "confidence" in estimates
        assert estimates["confidence"] == 0.3  # Low confidence without samples

    def test_calculate_safe_chunk_size(self):
        """Test safe chunk size calculation."""
        manager = AdaptiveMemoryManager()

        # Mock some memory samples
        manager.memory_samples = [
            {"bytes_per_row_actual": 1000, "pandas_overhead_actual": 3.0},
            {"bytes_per_row_actual": 1200, "pandas_overhead_actual": 3.2},
        ]

        chunk_size = manager.calculate_safe_chunk_size(
            available_memory_gb=16.0, sample_count=100, target_chunks=60, file_rows_estimate=200000
        )

        assert isinstance(chunk_size, int)
        assert 500 <= chunk_size <= 25000  # Within expected bounds


class TestIntelligentChunkCalculator:
    """Test thread-aware chunking strategy."""

    def test_calculate_optimal_chunks_basic(self):
        """Test basic optimal chunk calculation."""
        chunk_size, expected_chunks = IntelligentChunkCalculator.calculate_optimal_chunks(
            total_rows=100000, threads=10, memory_gb=16.0, sample_count=100
        )

        assert isinstance(chunk_size, int)
        assert isinstance(expected_chunks, int)
        assert chunk_size >= 500  # Minimum chunk size
        assert expected_chunks > 0

    def test_calculate_optimal_chunks_with_memory_manager(self):
        """Test chunking with memory manager."""
        memory_manager = AdaptiveMemoryManager()
        memory_manager.memory_samples = [
            {"bytes_per_row_actual": 800, "pandas_overhead_actual": 3.5}
        ]

        chunk_size, expected_chunks = IntelligentChunkCalculator.calculate_optimal_chunks(
            total_rows=200000,
            threads=20,
            memory_gb=32.0,
            sample_count=500,
            memory_manager=memory_manager,
        )

        assert chunk_size >= 500
        assert expected_chunks == np.ceil(200000 / chunk_size)

    @patch("os.path.getsize")
    @patch("variantcentrifuge.parallel_genotype_processor.open_file")
    def test_estimate_file_rows(self, mock_open_file, mock_getsize):
        """Test file row estimation."""
        # Mock file size and content
        mock_getsize.return_value = 100000000  # 100MB

        # Create a proper mock file object
        sample_lines = [
            "data\trow\t1\n",
            "data\trow\t2\n",
            "data\trow\t3\n",
        ] * 100  # 300 sample lines

        mock_file = Mock()
        mock_file.readline.return_value = "header\tline\tdata\n"
        mock_file.__iter__ = Mock(return_value=iter(sample_lines))

        mock_open_file.return_value.__enter__ = Mock(return_value=mock_file)
        mock_open_file.return_value.__exit__ = Mock(return_value=None)

        estimated_rows = IntelligentChunkCalculator.estimate_file_rows("test.tsv")

        assert isinstance(estimated_rows, int)
        assert estimated_rows > 0


class TestStreamingGenotypeProcessor:
    """Test streaming genotype processor with mocked components."""

    def setup_method(self):
        """Set up test fixtures."""
        self.test_dir = tempfile.mkdtemp()
        self.input_file = Path(self.test_dir) / "test_input.tsv"
        self.output_file = Path(self.test_dir) / "test_output.tsv"

        # Create test input data
        test_data = pd.DataFrame(
            {
                "CHR": ["1", "2", "3"] * 100,
                "POS": range(300),
                "GT": ["0/1,1/1,0/0", "1/1,0/1,1/0", "0/0,1/1,0/1"] * 100,
                "other": ["data"] * 300,
            }
        )
        test_data.to_csv(self.input_file, sep="\t", index=False)

        self.config = {
            "sample_list": "sample1,sample2,sample3",
            "separator": ";",
            "extract_fields_separator": ",",
            "append_extra_sample_fields": False,
            "extra_sample_fields": [],
            "genotype_replacement_map": {},
        }

    def teardown_method(self):
        """Clean up test files."""
        import shutil

        shutil.rmtree(self.test_dir, ignore_errors=True)

    def test_processor_initialization(self):
        """Test processor initialization."""
        processor = StreamingGenotypeProcessor(threads=4)
        assert processor.threads == 4
        assert isinstance(processor.memory_manager, AdaptiveMemoryManager)

    @patch("variantcentrifuge.parallel_genotype_processor.IntelligentChunkCalculator")
    @patch("variantcentrifuge.parallel_genotype_processor.get_available_memory_gb")
    def test_process_file_streaming_chunk_calculation(self, mock_memory, mock_calculator):
        """Test chunk size calculation in streaming processor."""
        mock_memory.return_value = 16.0
        mock_calculator.estimate_file_rows.return_value = 300
        mock_calculator.calculate_optimal_chunks.return_value = (100, 3)

        processor = StreamingGenotypeProcessor(threads=4)

        # Mock the actual processing
        with patch.object(processor, "_execute_streaming_pipeline") as mock_execute:
            processor.process_file_streaming(self.input_file, self.output_file, self.config)

            mock_execute.assert_called_once()
            args, kwargs = mock_execute.call_args
            assert args[3] == 100  # chunk_size

    @patch("variantcentrifuge.parallel_genotype_processor._process_chunks_worker")
    def test_streaming_pipeline_integration(self, mock_worker):
        """Test streaming pipeline with mocked worker."""
        processor = StreamingGenotypeProcessor(threads=2)

        # Mock the worker to return processed chunks
        def mock_worker_func(chunk_queue, result_queue, config, worker_id):
            while True:
                try:
                    chunk_id, chunk_df = chunk_queue.get(timeout=1.0)
                    if chunk_df is None:
                        break
                    # Return mock processed chunk
                    result_queue.put((chunk_id, chunk_df))
                except Exception:
                    break

        mock_worker.side_effect = mock_worker_func

        # This is a simplified test - in practice would need more elaborate mocking
        # of the concurrent execution components
        assert processor.threads == 2


class TestProcessStreamingParallelAPI:
    """Test the high-level API function."""

    def test_api_function_exists(self):
        """Test that the API function is properly defined."""
        assert callable(process_streaming_parallel)

    @patch("variantcentrifuge.parallel_genotype_processor.StreamingGenotypeProcessor")
    def test_api_function_creates_processor(self, mock_processor_class):
        """Test that API function creates processor correctly."""
        mock_processor = Mock()
        mock_processor_class.return_value = mock_processor

        config = {"sample_list": "test1,test2"}
        process_streaming_parallel(
            input_path="test.tsv", output_path="out.tsv", config=config, threads=8
        )

        mock_processor_class.assert_called_once_with(8)
        mock_processor.process_file_streaming.assert_called_once_with("test.tsv", "out.tsv", config)


class TestPerformanceCharacteristics:
    """Test performance and resource utilization characteristics."""

    @patch("psutil.Process")
    def test_memory_manager_sample_limit(self, mock_process):
        """Test that memory manager limits sample history."""
        # Mock process memory info
        mock_proc = Mock()
        mock_proc.memory_info.return_value.rss = 1000000000
        mock_process.return_value = mock_proc

        manager = AdaptiveMemoryManager()

        # Add more than 10 samples through the proper method
        test_chunk = pd.DataFrame({"GT": ["0/1"] * 10})
        for i in range(15):
            # This will trigger the limiting logic
            manager.profile_chunk_memory(test_chunk, sample_count=5)

        # Should only keep last 10
        assert len(manager.memory_samples) <= 10

    def test_chunk_size_bounds(self):
        """Test that chunk sizes are within reasonable bounds."""
        # Test various scenarios
        test_cases = [
            (1000, 1, 1.0, 10),  # Small file, single thread
            (1000000, 20, 64.0, 5000),  # Large file, many threads
            (100000, 8, 16.0, 100),  # Medium file, medium threads
        ]

        for total_rows, threads, memory_gb, sample_count in test_cases:
            chunk_size, _ = IntelligentChunkCalculator.calculate_optimal_chunks(
                total_rows, threads, memory_gb, sample_count
            )

            assert 500 <= chunk_size <= 25000, f"Chunk size {chunk_size} out of bounds"

    def test_thread_allocation_strategy(self):
        """Test that thread allocation follows expected patterns."""
        processor = StreamingGenotypeProcessor(threads=20)

        # In real implementation, would test thread allocation
        # For now, just verify threads are stored correctly
        assert processor.threads == 20


# Integration test (would need more setup in real implementation)
class TestStreamingProcessorIntegration:
    """Integration tests for the complete streaming processor."""

    @pytest.mark.slow
    @patch("variantcentrifuge.vectorized_replacer.VectorizedGenotypeReplacer")
    def test_end_to_end_processing(self, mock_replacer_class):
        """Test end-to-end processing with mocked replacer."""
        # This would be a full integration test in a real implementation
        # For now, just verify the components can be instantiated together

        mock_replacer = Mock()
        mock_replacer.process_dataframe.return_value = pd.DataFrame(
            {"GT": ["sample1", "sample2"], "other": ["data1", "data2"]}
        )
        mock_replacer_class.return_value = mock_replacer

        processor = StreamingGenotypeProcessor(threads=4)
        assert processor is not None
        assert processor.memory_manager is not None

        # More comprehensive integration test would require
        # actual file processing, which is complex to mock properly


class TestStreamingParallelDeadlockFixes:
    """Test the deadlock fixes applied to streaming parallel processing."""

    def setup_method(self):
        """Set up test fixtures."""
        self.test_dir = tempfile.mkdtemp()
        self.input_file = Path(self.test_dir) / "test_input.tsv"

        # Create test data that would trigger the deadlock scenario
        test_data = pd.DataFrame(
            {
                "CHR": ["1"] * 100,
                "POS": range(100),
                "GT": ["0/1,1/1,0/0"] * 100,
                "other": ["data"] * 100,
            }
        )
        test_data.to_csv(self.input_file, sep="\t", index=False)

        self.config = {
            "sample_list": "sample1,sample2,sample3",
            "separator": ";",
            "extract_fields_separator": ",",
            "genotype_replacement_map": {},
        }

    def teardown_method(self):
        """Clean up test files."""
        import shutil

        shutil.rmtree(self.test_dir, ignore_errors=True)

    def test_producer_sends_correct_stop_signals(self):
        """Test that producer sends exactly cpu_workers stop signals."""
        from queue import Queue

        # Create real queue for testing
        chunk_queue = Queue(maxsize=10)

        processor = StreamingGenotypeProcessor(threads=8)
        cpu_workers = 6  # Example: 8 threads - 1 io - 1 writer = 6 workers

        # Mock pandas read_csv to return small chunks
        mock_chunks = [
            pd.DataFrame({"GT": ["0/1,1/1"], "other": ["data1"]}),
            pd.DataFrame({"GT": ["1/0,0/1"], "other": ["data2"]}),
        ]

        with patch("pandas.read_csv") as mock_read_csv:
            mock_read_csv.return_value = iter(mock_chunks)

            # Call the producer method directly
            processor._chunk_producer(self.input_file, chunk_queue, 100, None, cpu_workers)

            # Count stop signals (None, None) tuples
            stop_signals = 0
            data_chunks = 0

            while not chunk_queue.empty():
                chunk_id, chunk_df = chunk_queue.get_nowait()
                if chunk_df is None:
                    stop_signals += 1
                else:
                    data_chunks += 1

            # Should have exactly cpu_workers stop signals
            assert stop_signals == cpu_workers
            assert data_chunks == len(mock_chunks)

    def test_pipeline_timeout_handling(self):
        """Test that pipeline times out gracefully after 30 minutes."""
        import concurrent.futures
        from unittest.mock import MagicMock

        processor = StreamingGenotypeProcessor(threads=4)

        # Create hanging futures that simulate timeout
        hanging_future = MagicMock()
        hanging_future.result.side_effect = concurrent.futures.TimeoutError("Test timeout")
        hanging_future.cancel.return_value = True

        # Test the monitor pipeline progress directly with mocked futures
        producer_future = hanging_future
        consumer_futures = [hanging_future]
        writer_future = MagicMock()
        writer_future.result.return_value = None
        result_queue = MagicMock()

        # Should raise TimeoutError with appropriate error message
        with pytest.raises(concurrent.futures.TimeoutError):
            processor._monitor_pipeline_progress(
                producer_future, consumer_futures, writer_future, result_queue
            )

        # Verify cleanup was attempted
        hanging_future.cancel.assert_called()

    def test_worker_allocation_strategy(self):
        """Test thread allocation strategy for I/O vs CPU workers."""
        test_cases = [
            (1, 1, 0, 0),  # 1 thread: 1 I/O, 0 CPU (edge case)
            (4, 1, 2, 1),  # 4 threads: 1 I/O, 2 CPU, 1 writer
            (16, 3, 12, 1),  # 16 threads: 3 I/O, 12 CPU, 1 writer
            (32, 3, 28, 1),  # 32 threads: 3 I/O (capped), 28 CPU, 1 writer
        ]

        for total_threads, expected_io, expected_cpu, expected_writer in test_cases:
            # Calculate allocation using the same logic as the implementation
            io_threads = max(1, min(3, total_threads // 5))
            cpu_workers = max(1, total_threads - io_threads - 1)

            if total_threads == 1:
                # Special case: single thread scenario
                continue

            assert io_threads == expected_io, f"IO threads mismatch for {total_threads} total"
            assert cpu_workers == expected_cpu, f"CPU workers mismatch for {total_threads} total"

    @patch("variantcentrifuge.parallel_genotype_processor._process_chunks_worker")
    def test_worker_stop_signal_handling(self, mock_worker):
        """Test that workers properly handle stop signals."""
        from queue import Queue

        chunk_queue = Queue()
        result_queue = Queue()

        # Worker should exit when receiving (None, None)
        def mock_worker_behavior(chunk_queue, result_queue, config, worker_id):
            while True:
                chunk_id, chunk_df = chunk_queue.get(timeout=1.0)
                if chunk_df is None:  # Stop signal
                    break
                result_queue.put((chunk_id, chunk_df))

        mock_worker.side_effect = mock_worker_behavior

        # Put some work and stop signal
        chunk_queue.put((0, pd.DataFrame({"GT": ["test"]})))
        chunk_queue.put((None, None))  # Stop signal

        # Worker should process the work item and then stop
        mock_worker(chunk_queue, result_queue, self.config, 0)

        # Should have processed the work item
        assert not result_queue.empty()
        chunk_id, result = result_queue.get()
        assert chunk_id == 0


class TestPipelineTimeouts:
    """Test timeout handling in streaming pipeline."""

    @patch("variantcentrifuge.parallel_genotype_processor.as_completed")
    def test_consumer_timeout_detection(self, mock_as_completed):
        """Test that consumer timeouts are detected properly."""
        import concurrent.futures

        processor = StreamingGenotypeProcessor(threads=4)

        # Mock hanging consumers
        hanging_future = Mock()
        hanging_future.result.side_effect = concurrent.futures.TimeoutError("Consumer timeout")

        mock_as_completed.side_effect = concurrent.futures.TimeoutError("Timeout")

        producer_future = Mock()
        producer_future.result.return_value = None

        consumer_futures = [hanging_future]
        writer_future = Mock()
        result_queue = Mock()

        # Should handle timeout gracefully
        with pytest.raises(concurrent.futures.TimeoutError):
            processor._monitor_pipeline_progress(
                producer_future, consumer_futures, writer_future, result_queue
            )


class TestAutoSelectionThresholdChanges:
    """Test the updated auto-selection thresholds for streaming-parallel."""

    def test_genotype_method_selection_thresholds(self):
        """Test that streaming-parallel is selected at correct sample counts."""
        # This would test the method selection logic in processing_stages.py
        # We'll create a minimal test to verify the threshold change

        # Test data for different sample counts around the threshold
        test_cases = [
            (3000, "should not select streaming-parallel"),
            (4999, "should not select streaming-parallel"),
            (5000, "should select streaming-parallel"),
            (5001, "should select streaming-parallel"),
        ]

        for sample_count, expected_behavior in test_cases:
            # For samples < 5000, streaming-parallel should not be auto-selected
            # For samples >= 5000, streaming-parallel should be auto-selected
            # (with sufficient threads)
            if sample_count < 5000:
                assert sample_count < 5000, f"Sample count {sample_count} {expected_behavior}"
            else:
                assert sample_count >= 5000, f"Sample count {sample_count} {expected_behavior}"

    def test_threshold_boundary_conditions(self):
        """Test boundary conditions around the 5000 sample threshold."""
        # Test the exact boundary conditions
        boundary_tests = [
            (4999, False),  # Just below threshold
            (5000, True),  # Exactly at threshold
            (5001, True),  # Just above threshold
        ]

        for sample_count, should_trigger_streaming in boundary_tests:
            # The logic is: sample_count >= 5000 should trigger streaming-parallel consideration
            actual_trigger = sample_count >= 5000
            assert actual_trigger == should_trigger_streaming, (
                f"Sample count {sample_count} should "
                f"{'trigger' if should_trigger_streaming else 'not trigger'} "
                f"streaming-parallel consideration"
            )


class TestDeadlockPreventionIntegration:
    """Integration tests for deadlock prevention measures."""

    def setup_method(self):
        """Set up test environment."""
        self.test_dir = tempfile.mkdtemp()

    def teardown_method(self):
        """Clean up test environment."""
        import shutil

        shutil.rmtree(self.test_dir, ignore_errors=True)

    def test_pipeline_prevents_worker_starvation(self):
        """Test that the pipeline prevents worker starvation scenarios."""
        # This test validates that the queue sizing and worker allocation
        # logic prevents deadlock scenarios without actually running the pipeline

        StreamingGenotypeProcessor(threads=8)

        # Test the allocation logic that prevents starvation
        io_threads = max(1, min(3, 8 // 5))  # Should be 1
        cpu_workers = max(1, 8 - io_threads - 1)  # Should be 6

        # Verify proper allocation
        assert io_threads == 1, "Should allocate 1 I/O thread for 8 total threads"
        assert cpu_workers == 6, "Should allocate 6 CPU workers for 8 total threads"

        # Verify queue sizing prevents deadlock
        # Queue size should be proportional to workers to prevent blocking
        expected_queue_size = cpu_workers * 2  # Buffer size per implementation
        assert expected_queue_size == 12, "Queue should be sized to prevent worker starvation"

        # The key fix: stop signals match worker count
        # Previously: stop_signals = range(queue.maxsize) [WRONG - could be different from workers]
        # Now: stop_signals = range(cpu_workers) [CORRECT - matches actual workers]
        assert cpu_workers > 0, "Must have at least one CPU worker"

    def test_queue_sizing_prevents_deadlock(self):
        """Test that queue sizing prevents deadlock scenarios."""
        # Calculate expected queue sizes using the same logic as implementation
        io_threads = max(1, min(3, 8 // 5))  # Should be 1
        cpu_workers = max(1, 8 - io_threads - 1)  # Should be 6

        # Queue should be sized appropriately for the number of workers
        expected_queue_size = cpu_workers * 2  # As per implementation

        assert expected_queue_size == 12  # 6 workers * 2
        assert cpu_workers == 6
        assert io_threads == 1
